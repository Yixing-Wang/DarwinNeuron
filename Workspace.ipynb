{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: iriica7w\n",
      "Sweep URL: https://wandb.ai/DarwinNeuron/Test/sweeps/iriica7w\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "device = 'cuda'\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        # awkward repeat\n",
    "        \"seed\": {\"values\":[1,2,3]},     \n",
    "        # Dataset:\n",
    "        \"nb_input\": {\"value\": 20},\n",
    "        \"nb_output\": {\"value\": 10},\n",
    "        \"nb_steps\": {\"value\": 50},\n",
    "        \"nb_data_samples\": {\"value\": 1000},\n",
    "        \"dim_manifold\": {\"value\": 2},\n",
    "        \"alpha\": {\"values\": [0.5, 1.0, 1.5, 2, 2.5]},  # Randman alpha parameter\n",
    "        # SNN:  \n",
    "        \"nb_hidden\": {\"value\": 10},\n",
    "        \"learn_beta\": {\"value\": False},        \n",
    "        # Training:\n",
    "        \"std\": {\"value\": 0.05},\n",
    "        \"epochs\": {\"value\":50},             \n",
    "        \"batch_size\": {\"value\": 256},\n",
    "        # Optimization:\n",
    "        \"loss_fn\": {\"value\": \"cross-entropy\"},\n",
    "        \"optimizer\": {\"value\": \"Adam\"},\n",
    "        \"lr\": {\"value\": 0.01},\n",
    "        \"regularization\": {\"value\": \"none\"},\n",
    "        # Evolution Strategy:\n",
    "        \"nb_model_samples\": {\"value\": 20}, \n",
    "        \"mirror\": {\"value\": True},\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, entity=\"DarwinNeuron\", project=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹Sweep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: icuh0v7u with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_manifold: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearn_beta: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_fn: cross-entropy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmirror: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_data_samples: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_hidden: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_input: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_model_samples: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_output: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_steps: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tregularization: none\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstd: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myixing\u001b[0m (\u001b[33mDarwinNeuron\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.5s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\w1886\\OneDrive\\My Documents\\Projects\\darwin_neuron\\wandb\\run-20250707_210026-icuh0v7u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DarwinNeuron/Test/runs/icuh0v7u' target=\"_blank\">pious-sweep-2</a></strong> to <a href='https://wandb.ai/DarwinNeuron/Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/DarwinNeuron/Test/sweeps/iriica7w' target=\"_blank\">https://wandb.ai/DarwinNeuron/Test/sweeps/iriica7w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DarwinNeuron/Test' target=\"_blank\">https://wandb.ai/DarwinNeuron/Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/DarwinNeuron/Test/sweeps/iriica7w' target=\"_blank\">https://wandb.ai/DarwinNeuron/Test/sweeps/iriica7w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DarwinNeuron/Test/runs/icuh0v7u' target=\"_blank\">https://wandb.ai/DarwinNeuron/Test/runs/icuh0v7u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "batch 0, loss: 2.340807, accuracy: 9.4%\n",
      "Accuracy: 11.2%, Avg loss: 2.331013 \n",
      "\n",
      "batch 1, loss: 2.335780, accuracy: 11.3%\n",
      "Accuracy: 12.5%, Avg loss: 2.326868 \n",
      "\n",
      "batch 2, loss: 2.330676, accuracy: 12.9%\n",
      "Accuracy: 12.8%, Avg loss: 2.322236 \n",
      "\n",
      "batch 3, loss: 2.312560, accuracy: 13.7%\n",
      "Accuracy: 12.3%, Avg loss: 2.321111 \n",
      "\n",
      "batch 4, loss: 2.324600, accuracy: 9.4%\n",
      "Accuracy: 12.4%, Avg loss: 2.318663 \n",
      "\n",
      "batch 5, loss: 2.311715, accuracy: 12.1%\n",
      "Accuracy: 12.3%, Avg loss: 2.316075 \n",
      "\n",
      "batch 6, loss: 2.322734, accuracy: 9.0%\n",
      "Accuracy: 11.9%, Avg loss: 2.313597 \n",
      "\n",
      "batch 7, loss: 2.327006, accuracy: 9.0%\n",
      "Accuracy: 11.6%, Avg loss: 2.312983 \n",
      "\n",
      "batch 8, loss: 2.302073, accuracy: 13.7%\n",
      "Accuracy: 11.5%, Avg loss: 2.312065 \n",
      "\n",
      "batch 9, loss: 2.296862, accuracy: 14.1%\n",
      "Accuracy: 11.5%, Avg loss: 2.310560 \n",
      "\n",
      "batch 10, loss: 2.310596, accuracy: 11.3%\n",
      "Accuracy: 11.5%, Avg loss: 2.309715 \n",
      "\n",
      "batch 11, loss: 2.310224, accuracy: 8.6%\n",
      "Accuracy: 11.2%, Avg loss: 2.309450 \n",
      "\n",
      "batch 12, loss: 2.317753, accuracy: 12.1%\n",
      "Accuracy: 11.3%, Avg loss: 2.308818 \n",
      "\n",
      "batch 13, loss: 2.304673, accuracy: 10.5%\n",
      "Accuracy: 11.3%, Avg loss: 2.308188 \n",
      "\n",
      "batch 14, loss: 2.302611, accuracy: 11.3%\n",
      "Accuracy: 11.4%, Avg loss: 2.307496 \n",
      "\n",
      "batch 15, loss: 2.297587, accuracy: 12.1%\n",
      "Accuracy: 11.4%, Avg loss: 2.305882 \n",
      "\n",
      "batch 16, loss: 2.310912, accuracy: 10.2%\n",
      "Accuracy: 11.3%, Avg loss: 2.305290 \n",
      "\n",
      "batch 17, loss: 2.302301, accuracy: 12.1%\n",
      "Accuracy: 11.2%, Avg loss: 2.305217 \n",
      "\n",
      "batch 18, loss: 2.300769, accuracy: 12.9%\n",
      "Accuracy: 11.2%, Avg loss: 2.304260 \n",
      "\n",
      "batch 19, loss: 2.296368, accuracy: 13.3%\n",
      "Accuracy: 11.5%, Avg loss: 2.303819 \n",
      "\n",
      "batch 20, loss: 2.299602, accuracy: 10.2%\n",
      "Accuracy: 11.6%, Avg loss: 2.303260 \n",
      "\n",
      "batch 21, loss: 2.302933, accuracy: 13.3%\n",
      "Accuracy: 11.6%, Avg loss: 2.302726 \n",
      "\n",
      "batch 22, loss: 2.304956, accuracy: 11.7%\n",
      "Accuracy: 11.6%, Avg loss: 2.302281 \n",
      "\n",
      "batch 23, loss: 2.301038, accuracy: 9.8%\n",
      "Accuracy: 11.7%, Avg loss: 2.301369 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303149 \n",
      "\n",
      "Accuracy: 11.2%, Avg loss: 2.303036 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.302982 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303079 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303346 \n",
      "\n",
      "Accuracy: 11.2%, Avg loss: 2.303640 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302916 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302785 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.303020 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302762 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.302737 \n",
      "\n",
      "Accuracy: 11.2%, Avg loss: 2.303127 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.302905 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302559 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.302627 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302568 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302411 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302144 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302080 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301978 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302012 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303111 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303091 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303036 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.302996 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.303124 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302788 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302826 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302643 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302680 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.302866 \n",
      "\n",
      "Accuracy: 11.2%, Avg loss: 2.303172 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302839 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302948 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302685 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.302527 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302553 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302334 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302240 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301812 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301633 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301441 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303243 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303131 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.303008 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.303077 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302290 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302464 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302517 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302815 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302892 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303015 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302888 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.303004 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302679 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.302618 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302605 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302227 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302105 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301858 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301629 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301287 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301419 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302884 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.303028 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302926 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302527 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302305 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302112 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302544 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302806 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303004 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302685 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303263 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.302738 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302496 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302598 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302233 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301889 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301713 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301506 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301563 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301611 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301515 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.303007 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302948 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302428 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302399 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302348 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302486 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302483 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302552 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302940 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.303266 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302816 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302652 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302565 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302339 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301926 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301764 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301588 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301573 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301594 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301365 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301553 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302823 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302248 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302291 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302625 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302440 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302755 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302430 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302531 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302549 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302576 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302682 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302441 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302416 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301798 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301716 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301657 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301781 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301638 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301175 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301781 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301879 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302233 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302312 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302576 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302699 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302617 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302495 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302692 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302433 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302168 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302324 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302166 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302294 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301808 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301825 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301976 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301870 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301338 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301313 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301867 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301869 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301991 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302306 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302808 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302854 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302495 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302524 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302892 \n",
      "\n",
      "Accuracy: 11.4%, Avg loss: 2.302510 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302381 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.302176 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301818 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301804 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301872 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302049 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301903 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301677 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301138 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301324 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301694 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301818 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.302186 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.301815 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302581 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.302573 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302474 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302557 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302539 \n",
      "\n",
      "Accuracy: 11.3%, Avg loss: 2.302752 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302539 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302319 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301916 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.301839 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301802 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301840 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302101 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301637 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301159 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301515 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301612 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301905 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.302178 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.301806 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301544 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.302680 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302402 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302574 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302582 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302702 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302386 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302137 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301963 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302110 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302175 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.302078 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301368 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301339 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301054 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301380 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301656 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301886 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.302252 \n",
      "\n",
      "Accuracy: 12.1%, Avg loss: 2.301541 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301318 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301167 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302563 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302678 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302638 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302851 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302153 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302081 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301943 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302095 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302402 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301986 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301369 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301215 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.300792 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301273 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301408 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.302103 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.301564 \n",
      "\n",
      "Accuracy: 12.1%, Avg loss: 2.301527 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.300871 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301267 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301513 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302619 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302897 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302398 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302136 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301950 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302079 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302291 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302526 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.302058 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301452 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301266 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.300828 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301093 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.300802 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.301346 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.301503 \n",
      "\n",
      "Accuracy: 12.1%, Avg loss: 2.301605 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301059 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.300858 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301622 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301980 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302404 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302431 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302135 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.301867 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302269 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302281 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302495 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301883 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301471 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301366 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.300680 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.300976 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301292 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.301402 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.301084 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301099 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.300902 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.300897 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301557 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301880 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301972 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302483 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302591 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302140 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302354 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.302381 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302368 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.301856 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301420 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301145 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301046 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301091 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301000 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.301309 \n",
      "\n",
      "Accuracy: 12.1%, Avg loss: 2.301158 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301074 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300519 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300830 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301151 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301473 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301933 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301883 \n",
      "\n",
      "Accuracy: 11.5%, Avg loss: 2.302708 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302210 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302592 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.302583 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302625 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.302085 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301694 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301212 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.300985 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301071 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301048 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.301178 \n",
      "\n",
      "Accuracy: 12.1%, Avg loss: 2.301309 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301024 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300770 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300663 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300675 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301494 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301850 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302011 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302448 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302554 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.302640 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.303027 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302675 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.302018 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301646 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301212 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301085 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301010 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301226 \n",
      "\n",
      "Accuracy: 12.1%, Avg loss: 2.301002 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301044 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300663 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300677 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300460 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300902 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301557 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301365 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301751 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.302001 \n",
      "\n",
      "Accuracy: 12.1%, Avg loss: 2.301953 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302785 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302804 \n",
      "\n",
      "Accuracy: 11.7%, Avg loss: 2.302378 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.302061 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301729 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301359 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301075 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301092 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301094 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.300849 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.300929 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300685 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300621 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300481 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301063 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301488 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301463 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301412 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301633 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302112 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302887 \n",
      "\n",
      "Accuracy: 11.6%, Avg loss: 2.302758 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.302082 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.302084 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301819 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301277 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301155 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301039 \n",
      "\n",
      "Accuracy: 12.1%, Avg loss: 2.300655 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301074 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301115 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300647 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300580 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300539 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301071 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301579 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301428 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301519 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301945 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302348 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302520 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.302722 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301773 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301986 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301681 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301277 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301178 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.300943 \n",
      "\n",
      "Accuracy: 12.1%, Avg loss: 2.300830 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301019 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.301244 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300680 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300742 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300785 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301054 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301435 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301366 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301817 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302107 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302229 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302915 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.302714 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.302544 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301685 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301413 \n",
      "\n",
      "Accuracy: 11.8%, Avg loss: 2.301253 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.300924 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.300885 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.300817 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.300898 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.300949 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300709 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300762 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300955 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301239 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.301368 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301700 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301747 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.302259 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302550 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302631 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302974 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.302913 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.303233 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301341 \n",
      "\n",
      "Accuracy: 11.9%, Avg loss: 2.301162 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.300595 \n",
      "\n",
      "Accuracy: 12.0%, Avg loss: 2.300671 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.300724 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.300977 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.300917 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300784 \n",
      "\n",
      "Accuracy: 12.5%, Avg loss: 2.300661 \n",
      "\n",
      "Accuracy: 12.4%, Avg loss: 2.300920 \n",
      "\n",
      "Accuracy: 12.5%, Avg loss: 2.301080 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301513 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301726 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.301555 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.302438 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302629 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.302705 \n",
      "\n",
      "Accuracy: 12.2%, Avg loss: 2.303037 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.302942 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.303259 \n",
      "\n",
      "Accuracy: 12.3%, Avg loss: 2.303502 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "batch 0, loss: 2.292994, accuracy: 12.1%\n",
      "Accuracy: 11.8%, Avg loss: 2.301118 \n",
      "\n",
      "batch 1, loss: 2.293066, accuracy: 11.3%\n",
      "Accuracy: 11.8%, Avg loss: 2.300476 \n",
      "\n",
      "batch 2, loss: 2.301060, accuracy: 10.5%\n",
      "Accuracy: 12.0%, Avg loss: 2.299473 \n",
      "\n",
      "batch 3, loss: 2.295375, accuracy: 16.0%\n",
      "Accuracy: 12.2%, Avg loss: 2.299340 \n",
      "\n",
      "batch 4, loss: 2.297400, accuracy: 11.7%\n",
      "Accuracy: 12.4%, Avg loss: 2.298792 \n",
      "\n",
      "batch 5, loss: 2.288251, accuracy: 15.6%\n",
      "Accuracy: 12.4%, Avg loss: 2.298485 \n",
      "\n",
      "batch 6, loss: 2.298832, accuracy: 10.5%\n",
      "Accuracy: 12.4%, Avg loss: 2.298003 \n",
      "\n",
      "batch 7, loss: 2.289118, accuracy: 11.7%\n",
      "Accuracy: 12.4%, Avg loss: 2.297653 \n",
      "\n",
      "batch 8, loss: 2.292889, accuracy: 12.5%\n",
      "Accuracy: 12.6%, Avg loss: 2.297042 \n",
      "\n",
      "batch 9, loss: 2.299536, accuracy: 13.3%\n",
      "Accuracy: 12.3%, Avg loss: 2.296940 \n",
      "\n",
      "batch 10, loss: 2.290206, accuracy: 10.2%\n",
      "Accuracy: 12.2%, Avg loss: 2.296201 \n",
      "\n",
      "batch 11, loss: 2.285734, accuracy: 16.4%\n",
      "Accuracy: 12.3%, Avg loss: 2.295249 \n",
      "\n",
      "batch 12, loss: 2.287634, accuracy: 11.3%\n",
      "Accuracy: 12.2%, Avg loss: 2.294960 \n",
      "\n",
      "batch 13, loss: 2.292222, accuracy: 13.7%\n",
      "Accuracy: 12.2%, Avg loss: 2.295007 \n",
      "\n",
      "batch 14, loss: 2.285494, accuracy: 13.7%\n",
      "Accuracy: 11.9%, Avg loss: 2.294719 \n",
      "\n",
      "batch 15, loss: 2.287654, accuracy: 10.9%\n",
      "Accuracy: 11.9%, Avg loss: 2.293652 \n",
      "\n",
      "batch 16, loss: 2.291283, accuracy: 14.1%\n",
      "Accuracy: 14.2%, Avg loss: 2.292888 \n",
      "\n",
      "batch 17, loss: 2.284289, accuracy: 15.2%\n",
      "Accuracy: 14.2%, Avg loss: 2.292897 \n",
      "\n",
      "batch 18, loss: 2.293991, accuracy: 11.3%\n",
      "Accuracy: 14.5%, Avg loss: 2.291641 \n",
      "\n",
      "batch 19, loss: 2.296597, accuracy: 13.3%\n",
      "Accuracy: 14.5%, Avg loss: 2.291064 \n",
      "\n",
      "batch 20, loss: 2.301825, accuracy: 14.1%\n",
      "Accuracy: 14.5%, Avg loss: 2.291095 \n",
      "\n",
      "batch 21, loss: 2.282747, accuracy: 16.4%\n",
      "Accuracy: 14.8%, Avg loss: 2.290161 \n",
      "\n",
      "batch 22, loss: 2.287119, accuracy: 17.2%\n",
      "Accuracy: 15.2%, Avg loss: 2.288796 \n",
      "\n",
      "batch 23, loss: 2.287298, accuracy: 11.6%\n",
      "Accuracy: 15.3%, Avg loss: 2.287984 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288934 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.289053 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288679 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288714 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288515 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288529 \n",
      "\n",
      "Accuracy: 14.9%, Avg loss: 2.288550 \n",
      "\n",
      "Accuracy: 14.9%, Avg loss: 2.288153 \n",
      "\n",
      "Accuracy: 14.9%, Avg loss: 2.288626 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288080 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287827 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287943 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287827 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288192 \n",
      "\n",
      "Accuracy: 14.9%, Avg loss: 2.288644 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288532 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288144 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287675 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287712 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287749 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287925 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.289124 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288735 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288553 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288551 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288583 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288291 \n",
      "\n",
      "Accuracy: 14.9%, Avg loss: 2.288228 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288008 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287913 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287870 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287631 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287873 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288257 \n",
      "\n",
      "Accuracy: 14.9%, Avg loss: 2.288769 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288279 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288120 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287033 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287505 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287640 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287885 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287786 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288570 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288455 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288315 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287971 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288165 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288138 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287957 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288007 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287799 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287353 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288019 \n",
      "\n",
      "Accuracy: 14.9%, Avg loss: 2.288473 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288429 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288338 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287130 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.286979 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287596 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287806 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287849 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287968 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288189 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288178 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288117 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287928 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288080 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287376 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287703 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287652 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287723 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287341 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288047 \n",
      "\n",
      "Accuracy: 14.9%, Avg loss: 2.288621 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288028 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287591 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287010 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287233 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287662 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287916 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288136 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288161 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288074 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287864 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288401 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288064 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288077 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287390 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288014 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287611 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287772 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287471 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288290 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287745 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287793 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287265 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287014 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287353 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287506 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288042 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287786 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287914 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288047 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288095 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288091 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288374 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287522 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287583 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287880 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287746 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288055 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287559 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287780 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287584 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287591 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.286908 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287183 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287374 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287613 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288353 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288123 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287919 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287965 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288021 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287875 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288050 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287748 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287713 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287925 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287962 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287797 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287543 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287619 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287581 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287580 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287142 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287432 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287385 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288024 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288538 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288187 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287851 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287871 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287965 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287904 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287840 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287803 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287847 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287972 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287823 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288016 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287545 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287760 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287615 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287614 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287260 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287608 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287709 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288286 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288428 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287898 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287884 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287906 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287930 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.287923 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287985 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287674 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287736 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287853 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288295 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287831 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.287876 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287633 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287605 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287706 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287475 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287842 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287866 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288108 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288144 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287891 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287994 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287797 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287987 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288265 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287808 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287699 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287779 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288092 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287720 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287535 \n",
      "\n",
      "Accuracy: 15.0%, Avg loss: 2.288137 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287620 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287607 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288074 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287700 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288044 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288440 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288252 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288277 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287735 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287969 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288201 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287919 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288056 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287840 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287675 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287949 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288274 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287704 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287878 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287838 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287554 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287763 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288321 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287862 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288711 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288154 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288393 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288266 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287984 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287845 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287874 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287905 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287920 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287864 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287616 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287791 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288142 \n",
      "\n",
      "Accuracy: 15.5%, Avg loss: 2.287192 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287241 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287530 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287494 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287759 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287943 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288027 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288594 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288450 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288605 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288327 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287973 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288095 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287765 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287873 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288157 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287698 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287803 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288425 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287906 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287268 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287177 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287343 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.286937 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287945 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288097 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288375 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288252 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288530 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288763 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288220 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287956 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287906 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287739 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287973 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288025 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287699 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287740 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288208 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287736 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287277 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287306 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287220 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287494 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287661 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287766 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288129 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288351 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288323 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288364 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288316 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288102 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287943 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287688 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288050 \n",
      "\n",
      "Accuracy: 15.1%, Avg loss: 2.288135 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287929 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287761 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287913 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287738 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287266 \n",
      "\n",
      "Accuracy: 15.5%, Avg loss: 2.287487 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287374 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287615 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287765 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288018 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288044 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287900 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288192 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288166 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287753 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288095 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288004 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288269 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288110 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288117 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288014 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287779 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287948 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287534 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287317 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287408 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287383 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287090 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287460 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287709 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287928 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288083 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287637 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287710 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.287786 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288027 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287694 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287673 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288528 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288292 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287949 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287933 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288162 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287803 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287593 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287537 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287070 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287381 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287522 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287600 \n",
      "\n",
      "Accuracy: 16.0%, Avg loss: 2.287271 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288111 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287759 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287735 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287405 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287488 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287389 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287904 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288119 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287917 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.288035 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.288178 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.288169 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287766 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287498 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287597 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287038 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287317 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287507 \n",
      "\n",
      "Accuracy: 15.9%, Avg loss: 2.287407 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287280 \n",
      "\n",
      "Accuracy: 16.0%, Avg loss: 2.286936 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287430 \n",
      "\n",
      "Accuracy: 15.2%, Avg loss: 2.288048 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287105 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287544 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287502 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287429 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287866 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287735 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287738 \n",
      "\n",
      "Accuracy: 15.5%, Avg loss: 2.287857 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.288161 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287410 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287654 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287702 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287558 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287353 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287695 \n",
      "\n",
      "Accuracy: 16.0%, Avg loss: 2.287203 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.286918 \n",
      "\n",
      "Accuracy: 15.9%, Avg loss: 2.286924 \n",
      "\n",
      "Accuracy: 16.1%, Avg loss: 2.286641 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287615 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287104 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287484 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287506 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287369 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287489 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287414 \n",
      "\n",
      "Accuracy: 15.3%, Avg loss: 2.287925 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.288180 \n",
      "\n",
      "Accuracy: 15.5%, Avg loss: 2.287676 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287266 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287807 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287524 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287400 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287596 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287786 \n",
      "\n",
      "Accuracy: 16.0%, Avg loss: 2.287038 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.286680 \n",
      "\n",
      "Accuracy: 16.0%, Avg loss: 2.286587 \n",
      "\n",
      "Accuracy: 16.2%, Avg loss: 2.285573 \n",
      "\n",
      "Accuracy: 16.2%, Avg loss: 2.285738 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.286813 \n",
      "\n",
      "Accuracy: 15.5%, Avg loss: 2.287436 \n",
      "\n",
      "Accuracy: 15.5%, Avg loss: 2.287305 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287566 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287598 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287344 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287608 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287582 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.288127 \n",
      "\n",
      "Accuracy: 15.5%, Avg loss: 2.287857 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287544 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287652 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287673 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287770 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287607 \n",
      "\n",
      "Accuracy: 15.9%, Avg loss: 2.286811 \n",
      "\n",
      "Accuracy: 16.0%, Avg loss: 2.286642 \n",
      "\n",
      "Accuracy: 16.1%, Avg loss: 2.285851 \n",
      "\n",
      "Accuracy: 16.1%, Avg loss: 2.285760 \n",
      "\n",
      "Accuracy: 16.2%, Avg loss: 2.285601 \n",
      "\n",
      "Accuracy: 16.1%, Avg loss: 2.285430 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287235 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287303 \n",
      "\n",
      "Accuracy: 15.5%, Avg loss: 2.287484 \n",
      "\n",
      "Accuracy: 15.5%, Avg loss: 2.287303 \n",
      "\n",
      "Accuracy: 15.5%, Avg loss: 2.287480 \n",
      "\n",
      "Accuracy: 15.5%, Avg loss: 2.287621 \n",
      "\n",
      "Accuracy: 15.4%, Avg loss: 2.287877 \n",
      "\n",
      "Accuracy: 15.6%, Avg loss: 2.287915 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287093 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287597 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287476 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287557 \n",
      "\n",
      "Accuracy: 15.7%, Avg loss: 2.287544 \n",
      "\n",
      "Accuracy: 15.8%, Avg loss: 2.287438 \n",
      "\n",
      "Accuracy: 15.9%, Avg loss: 2.286530 \n",
      "\n",
      "Accuracy: 16.1%, Avg loss: 2.286155 \n",
      "\n",
      "Accuracy: 16.1%, Avg loss: 2.285804 \n",
      "\n",
      "Accuracy: 16.1%, Avg loss: 2.285936 \n",
      "\n",
      "Accuracy: 16.2%, Avg loss: 2.285574 \n",
      "\n",
      "Accuracy: 16.2%, Avg loss: 2.285243 \n",
      "\n",
      "Accuracy: 16.2%, Avg loss: 2.284835 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "batch 0, loss: 2.278113, accuracy: 17.6%\n",
      "Accuracy: 15.4%, Avg loss: 2.287001 \n",
      "\n",
      "batch 1, loss: 2.287251, accuracy: 13.7%\n",
      "Accuracy: 15.7%, Avg loss: 2.285562 \n",
      "\n",
      "batch 2, loss: 2.273652, accuracy: 19.1%\n",
      "Accuracy: 16.0%, Avg loss: 2.284851 \n",
      "\n",
      "batch 3, loss: 2.288571, accuracy: 17.6%\n",
      "Accuracy: 16.1%, Avg loss: 2.283502 \n",
      "\n",
      "batch 4, loss: 2.283156, accuracy: 13.7%\n",
      "Accuracy: 16.4%, Avg loss: 2.281172 \n",
      "\n",
      "batch 5, loss: 2.278849, accuracy: 14.8%\n",
      "Accuracy: 16.4%, Avg loss: 2.279166 \n",
      "\n",
      "batch 6, loss: 2.268963, accuracy: 17.6%\n",
      "Accuracy: 16.7%, Avg loss: 2.277714 \n",
      "\n",
      "batch 7, loss: 2.276515, accuracy: 15.6%\n",
      "Accuracy: 16.9%, Avg loss: 2.276134 \n",
      "\n",
      "batch 8, loss: 2.277163, accuracy: 16.4%\n",
      "Accuracy: 17.0%, Avg loss: 2.275558 \n",
      "\n",
      "batch 9, loss: 2.266468, accuracy: 16.4%\n",
      "Accuracy: 17.2%, Avg loss: 2.273603 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, wandb, os\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from src.RandmanFunctions import read_randman_dataset, split_and_load\n",
    "from src.Models import RandmanSNN\n",
    "from src.EvolutionAlgorithms.EvolutionStrategy import ESModel\n",
    "from src.Training import train_loop_snn\n",
    "from src.Utilities import init_result_csv, set_seed\n",
    "from src.LandscapeAnalysis import LossSurfacePlotter\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "def run_config(config=None):\n",
    "    with torch.no_grad(), wandb.init(\n",
    "        config=config, \n",
    "    ) as run:        \n",
    "        config = wandb.config\n",
    "        \n",
    "        # update current run_name\n",
    "        keys = [\"seed\", \"std\", \"batch_size\", \"lr\", \"nb_model_samples\"]\n",
    "        sorted_items = [f\"{k}{getattr(config, k)}\" for k in sorted(keys)]\n",
    "        run.name = \"-\".join(sorted_items)\n",
    "        \n",
    "        # setting up local csv recording (optional)\n",
    "        config_dict = dict(run.config)\n",
    "        config_dict['run_id'] = run.id\n",
    "        result_path, _, _ = init_result_csv(config_dict, run.project)\n",
    "        \n",
    "        # initialize Evolution Strategy instance\n",
    "        set_seed(config.seed)\n",
    "        es_model = ESModel(\n",
    "            RandmanSNN,\n",
    "            config.nb_input,\n",
    "            config.nb_hidden,\n",
    "            config.nb_output,\n",
    "            0.95,\n",
    "            sample_size=config.nb_model_samples,\n",
    "            param_std=config.std,\n",
    "            Optimizer=optim.Adam,\n",
    "            lr=config.lr,\n",
    "            device=device,\n",
    "            mirror=config.mirror,\n",
    "        )\n",
    "\n",
    "        # load dataset        \n",
    "        train_loader, val_loader = split_and_load(read_randman_dataset(\n",
    "            run.config.nb_output,\n",
    "            run.config.nb_input,\n",
    "            run.config.nb_steps,\n",
    "            run.config.nb_data_samples,\n",
    "            run.config.dim_manifold,\n",
    "            run.config.alpha), run.config.batch_size)\n",
    "        \n",
    "        # loss surface plotter\n",
    "        plotter_dir = f\"results/{run.project}/runs/{run.id}/\"\n",
    "        os.makedirs(plotter_dir, exist_ok=True)\n",
    "        loss_plotter = LossSurfacePlotter(plotter_dir+\"illuminated_loss_surface.npz\")\n",
    "\n",
    "        # epochs\n",
    "        for epoch in range(config.epochs):\n",
    "            print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "\n",
    "            # train the model\n",
    "            train_loop_snn(es_model, train_loader, val_loader, cross_entropy, device, run, epoch, result_path, loss_plotter)\n",
    "\n",
    "# set sweep_id if picking up a sweep:\n",
    "sweep_id = 'iriica7w'\n",
    "wandb.agent(sweep_id, run_config, entity=\"DarwinNeuron\", project=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Run (for old people)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\w1886\\OneDrive\\My Documents\\Projects\\darwin_neuron\\wandb\\run-20250707_203212-ys7fex67</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DarwinNeuron/Test/runs/ys7fex67' target=\"_blank\">plotter-test</a></strong> to <a href='https://wandb.ai/DarwinNeuron/Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DarwinNeuron/Test' target=\"_blank\">https://wandb.ai/DarwinNeuron/Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DarwinNeuron/Test/runs/ys7fex67' target=\"_blank\">https://wandb.ai/DarwinNeuron/Test/runs/ys7fex67</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "batch 0, loss: 2.314266, accuracy: 8.2%\n",
      "Accuracy: 8.7%, Avg loss: 2.308529 \n",
      "\n",
      "batch 1, loss: 2.307100, accuracy: 8.6%\n",
      "Accuracy: 8.8%, Avg loss: 2.307101 \n",
      "\n",
      "batch 2, loss: 2.302372, accuracy: 9.4%\n",
      "Accuracy: 9.0%, Avg loss: 2.305346 \n",
      "\n",
      "batch 3, loss: 2.305645, accuracy: 13.7%\n",
      "Accuracy: 9.4%, Avg loss: 2.303466 \n",
      "\n",
      "batch 4, loss: 2.305126, accuracy: 8.2%\n",
      "Accuracy: 9.5%, Avg loss: 2.303110 \n",
      "\n",
      "batch 5, loss: 2.306287, accuracy: 10.2%\n",
      "Accuracy: 9.8%, Avg loss: 2.301988 \n",
      "\n",
      "batch 6, loss: 2.308053, accuracy: 9.0%\n",
      "Accuracy: 9.9%, Avg loss: 2.302093 \n",
      "\n",
      "batch 7, loss: 2.306416, accuracy: 9.0%\n",
      "Accuracy: 10.1%, Avg loss: 2.301707 \n",
      "\n",
      "batch 8, loss: 2.305209, accuracy: 10.5%\n",
      "Accuracy: 10.1%, Avg loss: 2.301621 \n",
      "\n",
      "batch 9, loss: 2.301850, accuracy: 10.9%\n",
      "Accuracy: 10.1%, Avg loss: 2.301789 \n",
      "\n",
      "batch 10, loss: 2.299750, accuracy: 8.2%\n",
      "Accuracy: 10.2%, Avg loss: 2.301653 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\w1886\\AppData\\Local\\Temp\\ipykernel_29544\\3703992940.py\", line 88, in train_snn\n",
      "    train_loop_snn(es_model, train_loader, val_loader, cross_entropy, device, run, epoch, loss_plotter=loss_plotter)\n",
      "  File \"c:\\Users\\w1886\\OneDrive\\My Documents\\Projects\\darwin_neuron\\src\\Training.py\", line 66, in train_loop_snn\n",
      "    log_model(es_model, run)\n",
      "  File \"c:\\Users\\w1886\\OneDrive\\My Documents\\Projects\\darwin_neuron\\src\\Training.py\", line 15, in log_model\n",
      "    run.log_model(path=filename, name=model_name)\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 398, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 456, in wrapper_fn\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 443, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 3419, in log_model\n",
      "    self._log_artifact(\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 3261, in _log_artifact\n",
      "    self._assert_can_log_artifact(artifact)\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 3312, in _assert_can_log_artifact\n",
      "    expected_type = Artifact._expected_type(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\sdk\\artifacts\\artifact.py\", line 2695, in _expected_type\n",
      "    response = client.execute(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\sdk\\lib\\retry.py\", line 211, in wrapped_fn\n",
      "    return retrier(*args, **kargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\sdk\\lib\\retry.py\", line 134, in __call__\n",
      "    result = self._call_fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\apis\\public\\api.py\", line 110, in execute\n",
      "    return self._client.execute(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\client.py\", line 52, in execute\n",
      "    result = self._get_result(document, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\client.py\", line 60, in _get_result\n",
      "    return self.transport.execute(document, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\wandb\\sdk\\lib\\gql_request.py\", line 58, in execute\n",
      "    request = self.session.post(self.url, **post_args)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\requests\\sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\urllib3\\connection.py\", line 790, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\urllib3\\connection.py\", line 969, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\urllib3\\util\\ssl_.py\", line 480, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\urllib3\\util\\ssl_.py\", line 524, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\ssl.py\", line 1382, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "uploading artifact ys7fex67-batch-log.pth (1.5m)<br>uploading artifact ys7fex67-batch-log.pth (1.4m)<br>uploading artifact ys7fex67-batch-log.pth (1.3m)<br>uploading artifact ys7fex67-batch-log.pth (1.1m)<br>uploading artifact ys7fex67-batch-log.pth (59s)<br>+ 1 more task(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch, wandb, os\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from src.RandmanFunctions import read_randman_dataset, split_and_load\n",
    "from src.Models import RandmanSNN\n",
    "from src.EvolutionAlgorithms.EvolutionStrategy import ESModel\n",
    "from src.Training import train_loop_snn\n",
    "from src.Utilities import init_result_csv, set_seed\n",
    "from src.LandscapeAnalysis import LossSurfacePlotter\n",
    "\n",
    "device = 'cuda'\n",
    "@torch.no_grad()\n",
    "def train_snn():\n",
    "    run_name = \"plotter-test\"\n",
    "    config = {  # Dataset:\n",
    "        \"nb_input\": 10,\n",
    "        \"nb_output\": 10,\n",
    "        \"nb_steps\": 50,\n",
    "        \"nb_data_samples\": 1000,\n",
    "        \"dim_manifold\": 2,\n",
    "        \"alpha\": 2.0,\n",
    "        # SNN:\n",
    "        \"nb_hidden\": 10,\n",
    "        \"learn_beta\": False,\n",
    "        # Evolution Strategy:\n",
    "        \"nb_model_samples\": 20,\n",
    "        \"mirror\": True,\n",
    "        # Training:\n",
    "        \"std\": 0.05,\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 256,\n",
    "        # Optimization:\n",
    "        \"loss\": \"cross-entropy\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"lr\": 0.01,\n",
    "        \"regularization\": \"none\",\n",
    "    }\n",
    "    with wandb.init(\n",
    "        entity=\"DarwinNeuron\", project=\"Test\", name=run_name, config=config\n",
    "    ) as run:\n",
    "        # update current run_name\n",
    "        keys = [\"std\", \"batch_size\", \"lr\", \"nb_model_samples\"]\n",
    "        sorted_items = [f\"{k}{getattr(run.config, k)}\" for k in sorted(keys)]\n",
    "        run.name = \"-\".join(sorted_items)\n",
    "        \n",
    "        # setting up local csv recording (optional)\n",
    "        result_path, _, _ = init_result_csv(dict(run.config), run.project)\n",
    "\n",
    "        # initialize Evolution Strategy instance\n",
    "        es_model = ESModel(\n",
    "            RandmanSNN,\n",
    "            run.config.nb_input,\n",
    "            run.config.nb_hidden,\n",
    "            run.config.nb_output,\n",
    "            0.95,\n",
    "            sample_size=run.config.nb_model_samples,\n",
    "            param_std=run.config.std,\n",
    "            Optimizer=optim.Adam,\n",
    "            lr=run.config.lr,\n",
    "            device=device,\n",
    "            mirror=run.config.mirror,\n",
    "        )\n",
    "\n",
    "        # load dataset\n",
    "        train_loader, val_loader = split_and_load(\n",
    "            read_randman_dataset(\n",
    "                run.config.nb_output,\n",
    "                run.config.nb_input,\n",
    "                run.config.nb_steps,\n",
    "                run.config.nb_data_samples,\n",
    "                run.config.dim_manifold,\n",
    "                run.config.alpha,\n",
    "            ),\n",
    "            run.config.batch_size,\n",
    "        )\n",
    "        \n",
    "        # loss surface plotter\n",
    "        plotter_dir = f\"results/{run.project}/runs/{run.id}/\"\n",
    "        os.makedirs(plotter_dir, exist_ok=True)\n",
    "        loss_plotter = LossSurfacePlotter(plotter_dir+\"illuminated_loss_surface.npz\")\n",
    "\n",
    "        # epochs\n",
    "        for epoch in range(run.config.epochs):\n",
    "            print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "\n",
    "            # train the model\n",
    "            train_loop_snn(es_model, train_loader, val_loader, cross_entropy, device, run, epoch, loss_plotter=loss_plotter)\n",
    "\n",
    "train_snn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a plot-builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2) (2,) newland.npz 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import cKDTree\n",
    "from src.LandscapeAnalysis import get_orthogonal_vectors, illuminate_loss_surface_2d\n",
    "import plotly.graph_objects as go\n",
    "from src.LandscapeAnalysis import LossSurfacePlotter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def ball(x):\n",
    "    return np.sqrt(np.sum(x**2))\n",
    "def test():\n",
    "    center = np.array([0.0, 0.0])\n",
    "    resolution = 20\n",
    "    range_lim = 1\n",
    "    \n",
    "    lsp = LossSurfacePlotter()\n",
    "    lsp.illuminate_2d(center, ball,range_lim, resolution)\n",
    "    \n",
    "    # add another center\n",
    "    center2 = np.array([10, 0])\n",
    "    lsp.illuminate_2d(center2, ball, range_lim, resolution)\n",
    "    \n",
    "    fig = lsp.get_plot(resolution=512)\n",
    "    fig.show()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from numpy.random import rand\n",
    "import numpy as np\n",
    "\n",
    "def try_cKDTree():\n",
    "    points = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "    tree = cKDTree(points)\n",
    "    \n",
    "    query = np.array([[0,0],[0.5,1]])\n",
    "    distances, _ = tree.query(query,k=1)\n",
    "    print(distances)\n",
    "try_cKDTree()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
