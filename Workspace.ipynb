{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "import wandb\n",
    "\n",
    "from src.RandmanFunctions import read_randman10_dataset\n",
    "from src.Models import RandmanSNN\n",
    "from src.EvolutionAlgorithms.EvolutionStrategy import ESModel\n",
    "from src.Training import train_loop_snn\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 78qhh6zn\n",
      "Sweep URL: https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {        \n",
    "        # Dataset:\n",
    "        \"nb_input\": {\"value\": 100},\n",
    "        \"nb_output\": {\"value\": 10},\n",
    "        \"nb_steps\": {\"value\": 50},\n",
    "        \"nb_data_samples\": {\"value\": 1000},\n",
    "        # SNN:\n",
    "        \"nb_hidden\": {\"value\": 10},\n",
    "        \"learn_beta\": {\"value\": False},        \n",
    "        # Training:\n",
    "        \"std\": {\"values\": [0.05, 0.1]},\n",
    "        \"epochs\": {\"value\":50},\n",
    "        \"batch_size\": {\"values\": [64, 256]},\n",
    "        # Optimization:\n",
    "        \"loss_fn\": {\"value\": \"cross-entropy\"},\n",
    "        \"optimizer\": {\"value\": \"Adam\"},\n",
    "        \"lr\": {\"value\": 0.01},\n",
    "        \"regularization\": {\"value\": \"none\"},\n",
    "        # Evolution Strategy:\n",
    "        \"nb_model_samples\": {\"value\": 20},\n",
    "        \"mirror\": {\"value\": True},\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"ES-Randman10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπSweep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_snn(config=None):\n",
    "    with torch.no_grad(), wandb.init(\n",
    "        config=config\n",
    "    ) as run:\n",
    "        config = wandb.config\n",
    "        # initialize Evolution Strategy instance\n",
    "        es_model = ESModel(\n",
    "            RandmanSNN,\n",
    "            config.nb_input,\n",
    "            config.nb_hidden,\n",
    "            config.nb_output,\n",
    "            0.95,\n",
    "            sample_size=config.nb_model_samples,\n",
    "            param_std=config.std,\n",
    "            Optimizer=optim.Adam,\n",
    "            lr=config.lr,\n",
    "            device=device,\n",
    "            mirror=config.mirror,\n",
    "        )\n",
    "\n",
    "        # load dataset\n",
    "        train_loader, val_loader = read_randman10_dataset(\n",
    "            \"data/randman_10_dataset.pt\", batch_size=config.batch_size\n",
    "        )\n",
    "\n",
    "        # epochs\n",
    "        for epoch in range(config.epochs):\n",
    "            print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "\n",
    "            # train the model\n",
    "            train_loop_snn(\n",
    "            es_model, train_loader, val_loader, cross_entropy, device, run\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6ri90zn0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearn_beta: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_fn: cross-entropy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmirror: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_data_samples: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_hidden: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_input: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_model_samples: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_output: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_steps: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tregularization: none\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstd: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myixing\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.3s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wyx/darwin_neuron/wandb/run-20250617_191102-6ri90zn0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yixing/ES-Randman10/runs/6ri90zn0' target=\"_blank\">stellar-sweep-1</a></strong> to <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yixing/ES-Randman10/runs/6ri90zn0' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/runs/6ri90zn0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "batch 0, loss: 2.332244, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 11.7%, Avg loss: 2.331179 \n",
      "\n",
      "batch 1, loss: 2.342872, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 11.1%, Avg loss: 2.328147 \n",
      "\n",
      "batch 2, loss: 2.351147, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 11.3%, Avg loss: 2.322961 \n",
      "\n",
      "batch 3, loss: 2.321573, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 11.0%, Avg loss: 2.319260 \n",
      "\n",
      "batch 4, loss: 2.310365, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 11.5%, Avg loss: 2.317127 \n",
      "\n",
      "batch 5, loss: 2.283870, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 11.3%, Avg loss: 2.314515 \n",
      "\n",
      "batch 6, loss: 2.337767, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.1%, Avg loss: 2.313232 \n",
      "\n",
      "batch 7, loss: 2.326154, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 10.8%, Avg loss: 2.312222 \n",
      "\n",
      "batch 8, loss: 2.309592, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 10.8%, Avg loss: 2.310760 \n",
      "\n",
      "batch 9, loss: 2.311606, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 10.3%, Avg loss: 2.310198 \n",
      "\n",
      "batch 10, loss: 2.304059, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 10.4%, Avg loss: 2.309631 \n",
      "\n",
      "batch 11, loss: 2.317556, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 10.2%, Avg loss: 2.309859 \n",
      "\n",
      "batch 12, loss: 2.307311, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 10.3%, Avg loss: 2.308716 \n",
      "\n",
      "batch 13, loss: 2.322410, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 10.4%, Avg loss: 2.308532 \n",
      "\n",
      "batch 14, loss: 2.289612, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 10.4%, Avg loss: 2.307896 \n",
      "\n",
      "batch 15, loss: 2.282344, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 10.2%, Avg loss: 2.308029 \n",
      "\n",
      "batch 16, loss: 2.309733, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 10.0%, Avg loss: 2.307077 \n",
      "\n",
      "batch 17, loss: 2.322976, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 10.0%, Avg loss: 2.307312 \n",
      "\n",
      "batch 18, loss: 2.326110, accuracy: 4.7%\n",
      "Test Error: \n",
      "Accuracy: 9.8%, Avg loss: 2.308119 \n",
      "\n",
      "batch 19, loss: 2.305297, accuracy: 3.1%\n",
      "Test Error: \n",
      "Accuracy: 9.6%, Avg loss: 2.307523 \n",
      "\n",
      "batch 20, loss: 2.341000, accuracy: 3.1%\n",
      "Test Error: \n",
      "Accuracy: 9.6%, Avg loss: 2.306415 \n",
      "\n",
      "batch 21, loss: 2.288085, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 9.4%, Avg loss: 2.305401 \n",
      "\n",
      "batch 22, loss: 2.302458, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 9.3%, Avg loss: 2.304710 \n",
      "\n",
      "batch 23, loss: 2.306072, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 9.3%, Avg loss: 2.305189 \n",
      "\n",
      "batch 24, loss: 2.314654, accuracy: 3.1%\n",
      "Test Error: \n",
      "Accuracy: 9.7%, Avg loss: 2.303675 \n",
      "\n",
      "batch 25, loss: 2.312838, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 9.8%, Avg loss: 2.302810 \n",
      "\n",
      "batch 26, loss: 2.275200, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 10.0%, Avg loss: 2.302339 \n",
      "\n",
      "batch 27, loss: 2.292917, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 9.8%, Avg loss: 2.302809 \n",
      "\n",
      "batch 28, loss: 2.290894, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 9.8%, Avg loss: 2.302296 \n",
      "\n",
      "batch 29, loss: 2.261864, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 9.7%, Avg loss: 2.303178 \n",
      "\n",
      "batch 30, loss: 2.295928, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 10.0%, Avg loss: 2.303012 \n",
      "\n",
      "batch 31, loss: 2.294771, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 10.2%, Avg loss: 2.303120 \n",
      "\n",
      "batch 32, loss: 2.297436, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 10.3%, Avg loss: 2.302349 \n",
      "\n",
      "batch 33, loss: 2.322315, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 10.2%, Avg loss: 2.301801 \n",
      "\n",
      "batch 34, loss: 2.279748, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 10.3%, Avg loss: 2.301069 \n",
      "\n",
      "batch 35, loss: 2.309330, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 10.5%, Avg loss: 2.300423 \n",
      "\n",
      "batch 36, loss: 2.302653, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 10.2%, Avg loss: 2.301689 \n",
      "\n",
      "batch 37, loss: 2.282908, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 10.1%, Avg loss: 2.301219 \n",
      "\n",
      "batch 38, loss: 2.269947, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 10.4%, Avg loss: 2.301027 \n",
      "\n",
      "batch 39, loss: 2.272137, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 10.9%, Avg loss: 2.299342 \n",
      "\n",
      "batch 40, loss: 2.283013, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 11.2%, Avg loss: 2.297786 \n",
      "\n",
      "batch 41, loss: 2.304360, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 11.2%, Avg loss: 2.296692 \n",
      "\n",
      "batch 42, loss: 2.328298, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 10.8%, Avg loss: 2.296879 \n",
      "\n",
      "batch 43, loss: 2.324000, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.2%, Avg loss: 2.296169 \n",
      "\n",
      "batch 44, loss: 2.301110, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.5%, Avg loss: 2.295536 \n",
      "\n",
      "batch 45, loss: 2.281345, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 11.3%, Avg loss: 2.294310 \n",
      "\n",
      "batch 46, loss: 2.284478, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 11.5%, Avg loss: 2.294300 \n",
      "\n",
      "batch 47, loss: 2.294982, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 11.3%, Avg loss: 2.293646 \n",
      "\n",
      "batch 48, loss: 2.283814, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 11.5%, Avg loss: 2.292736 \n",
      "\n",
      "batch 49, loss: 2.286168, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 11.2%, Avg loss: 2.292130 \n",
      "\n",
      "batch 50, loss: 2.297920, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.289861 \n",
      "\n",
      "batch 51, loss: 2.252786, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.288856 \n",
      "\n",
      "batch 52, loss: 2.287022, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 12.6%, Avg loss: 2.287080 \n",
      "\n",
      "batch 53, loss: 2.248148, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 12.6%, Avg loss: 2.285332 \n",
      "\n",
      "batch 54, loss: 2.277469, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 12.7%, Avg loss: 2.284115 \n",
      "\n",
      "batch 55, loss: 2.297868, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 12.7%, Avg loss: 2.283015 \n",
      "\n",
      "batch 56, loss: 2.303632, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.280422 \n",
      "\n",
      "batch 57, loss: 2.265203, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 13.7%, Avg loss: 2.278565 \n",
      "\n",
      "batch 58, loss: 2.301054, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 13.7%, Avg loss: 2.277598 \n",
      "\n",
      "batch 59, loss: 2.305530, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 14.1%, Avg loss: 2.275778 \n",
      "\n",
      "batch 60, loss: 2.293273, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 14.1%, Avg loss: 2.274519 \n",
      "\n",
      "batch 61, loss: 2.286577, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 14.2%, Avg loss: 2.272326 \n",
      "\n",
      "batch 62, loss: 2.272618, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 14.1%, Avg loss: 2.270253 \n",
      "\n",
      "batch 63, loss: 2.293953, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 14.1%, Avg loss: 2.269104 \n",
      "\n",
      "batch 64, loss: 2.307243, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 13.1%, Avg loss: 2.269213 \n",
      "\n",
      "batch 65, loss: 2.280016, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.268517 \n",
      "\n",
      "batch 66, loss: 2.257749, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 13.3%, Avg loss: 2.268362 \n",
      "\n",
      "batch 67, loss: 2.238954, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 13.6%, Avg loss: 2.265316 \n",
      "\n",
      "batch 68, loss: 2.265054, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 13.8%, Avg loss: 2.265419 \n",
      "\n",
      "batch 69, loss: 2.274651, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 14.4%, Avg loss: 2.261534 \n",
      "\n",
      "batch 70, loss: 2.279192, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 14.5%, Avg loss: 2.259186 \n",
      "\n",
      "batch 71, loss: 2.253305, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 14.9%, Avg loss: 2.256361 \n",
      "\n",
      "batch 72, loss: 2.275105, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 15.3%, Avg loss: 2.253327 \n",
      "\n",
      "batch 73, loss: 2.299664, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 15.3%, Avg loss: 2.250269 \n",
      "\n",
      "batch 74, loss: 2.257480, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 15.7%, Avg loss: 2.247502 \n",
      "\n",
      "batch 75, loss: 2.220478, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 15.5%, Avg loss: 2.246903 \n",
      "\n",
      "batch 76, loss: 2.240356, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 15.7%, Avg loss: 2.244071 \n",
      "\n",
      "batch 77, loss: 2.217195, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 16.2%, Avg loss: 2.242722 \n",
      "\n",
      "batch 78, loss: 2.228021, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 16.8%, Avg loss: 2.239054 \n",
      "\n",
      "batch 79, loss: 2.241015, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 16.6%, Avg loss: 2.237342 \n",
      "\n",
      "batch 80, loss: 2.257486, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 15.7%, Avg loss: 2.240656 \n",
      "\n",
      "batch 81, loss: 2.303990, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 15.9%, Avg loss: 2.240610 \n",
      "\n",
      "batch 82, loss: 2.251426, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 16.4%, Avg loss: 2.237411 \n",
      "\n",
      "batch 83, loss: 2.195455, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 17.2%, Avg loss: 2.236044 \n",
      "\n",
      "batch 84, loss: 2.244354, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 17.6%, Avg loss: 2.233934 \n",
      "\n",
      "batch 85, loss: 2.216449, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 17.8%, Avg loss: 2.233974 \n",
      "\n",
      "batch 86, loss: 2.228646, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 17.6%, Avg loss: 2.234349 \n",
      "\n",
      "batch 87, loss: 2.269388, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 17.5%, Avg loss: 2.232937 \n",
      "\n",
      "batch 88, loss: 2.186068, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 17.5%, Avg loss: 2.233640 \n",
      "\n",
      "batch 89, loss: 2.188641, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 18.1%, Avg loss: 2.234274 \n",
      "\n",
      "batch 90, loss: 2.209824, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 18.1%, Avg loss: 2.231239 \n",
      "\n",
      "batch 91, loss: 2.196209, accuracy: 23.4%\n",
      "Test Error: \n",
      "Accuracy: 18.4%, Avg loss: 2.229454 \n",
      "\n",
      "batch 92, loss: 2.267419, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 19.0%, Avg loss: 2.226988 \n",
      "\n",
      "batch 93, loss: 2.229836, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 19.4%, Avg loss: 2.226454 \n",
      "\n",
      "batch 94, loss: 2.246628, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 19.4%, Avg loss: 2.222204 \n",
      "\n",
      "batch 95, loss: 2.251177, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 19.4%, Avg loss: 2.219811 \n",
      "\n",
      "batch 96, loss: 2.260045, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 19.9%, Avg loss: 2.216865 \n",
      "\n",
      "batch 97, loss: 2.214974, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 20.8%, Avg loss: 2.213261 \n",
      "\n",
      "batch 98, loss: 2.216284, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 20.7%, Avg loss: 2.210449 \n",
      "\n",
      "batch 99, loss: 2.197835, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 21.0%, Avg loss: 2.208310 \n",
      "\n",
      "batch 100, loss: 2.150672, accuracy: 23.4%\n",
      "Test Error: \n",
      "Accuracy: 21.4%, Avg loss: 2.205273 \n",
      "\n",
      "batch 101, loss: 2.125889, accuracy: 28.1%\n",
      "Test Error: \n",
      "Accuracy: 21.3%, Avg loss: 2.204016 \n",
      "\n",
      "batch 102, loss: 2.244514, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 21.7%, Avg loss: 2.202272 \n",
      "\n",
      "batch 103, loss: 2.123333, accuracy: 26.6%\n",
      "Test Error: \n",
      "Accuracy: 22.0%, Avg loss: 2.200716 \n",
      "\n",
      "batch 104, loss: 2.176975, accuracy: 26.6%\n",
      "Test Error: \n",
      "Accuracy: 22.8%, Avg loss: 2.199306 \n",
      "\n",
      "batch 105, loss: 2.189175, accuracy: 23.4%\n",
      "Test Error: \n",
      "Accuracy: 22.6%, Avg loss: 2.196269 \n",
      "\n",
      "batch 106, loss: 2.135231, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 22.8%, Avg loss: 2.194496 \n",
      "\n",
      "batch 107, loss: 2.128367, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 22.0%, Avg loss: 2.194059 \n",
      "\n",
      "batch 108, loss: 2.240532, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 21.6%, Avg loss: 2.192732 \n",
      "\n",
      "batch 109, loss: 2.172911, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 21.8%, Avg loss: 2.189315 \n",
      "\n",
      "batch 110, loss: 2.118099, accuracy: 23.4%\n",
      "Test Error: \n",
      "Accuracy: 23.1%, Avg loss: 2.184243 \n",
      "\n",
      "batch 111, loss: 2.180116, accuracy: 25.0%\n",
      "Test Error: \n",
      "Accuracy: 23.5%, Avg loss: 2.179976 \n",
      "\n",
      "batch 112, loss: 2.148124, accuracy: 28.1%\n",
      "Test Error: \n",
      "Accuracy: 23.8%, Avg loss: 2.177639 \n",
      "\n",
      "batch 113, loss: 2.084025, accuracy: 25.0%\n",
      "Test Error: \n",
      "Accuracy: 23.9%, Avg loss: 2.172330 \n",
      "\n",
      "batch 114, loss: 2.090134, accuracy: 29.7%\n",
      "Test Error: \n",
      "Accuracy: 24.2%, Avg loss: 2.169432 \n",
      "\n",
      "batch 115, loss: 2.089186, accuracy: 26.6%\n",
      "Test Error: \n",
      "Accuracy: 24.5%, Avg loss: 2.168634 \n",
      "\n",
      "batch 116, loss: 2.196384, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 24.4%, Avg loss: 2.168211 \n",
      "\n",
      "batch 117, loss: 2.037080, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 24.7%, Avg loss: 2.168201 \n",
      "\n",
      "batch 118, loss: 2.057117, accuracy: 34.4%\n",
      "Test Error: \n",
      "Accuracy: 24.6%, Avg loss: 2.165657 \n",
      "\n",
      "batch 119, loss: 2.099842, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 24.2%, Avg loss: 2.160752 \n",
      "\n",
      "batch 120, loss: 2.110258, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 23.9%, Avg loss: 2.159403 \n",
      "\n",
      "batch 121, loss: 2.103756, accuracy: 34.4%\n",
      "Test Error: \n",
      "Accuracy: 24.2%, Avg loss: 2.155960 \n",
      "\n",
      "batch 122, loss: 2.093339, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 24.8%, Avg loss: 2.151201 \n",
      "\n",
      "batch 123, loss: 2.092296, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 24.9%, Avg loss: 2.147345 \n",
      "\n",
      "batch 124, loss: 2.246348, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 25.4%, Avg loss: 2.145736 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "batch 0, loss: 2.164066, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 26.1%, Avg loss: 2.139922 \n",
      "\n",
      "batch 1, loss: 2.080362, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 26.8%, Avg loss: 2.133033 \n",
      "\n",
      "batch 2, loss: 2.067344, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 26.9%, Avg loss: 2.129100 \n",
      "\n",
      "batch 3, loss: 2.105669, accuracy: 37.5%\n",
      "Test Error: \n",
      "Accuracy: 27.1%, Avg loss: 2.125117 \n",
      "\n",
      "batch 4, loss: 2.043477, accuracy: 28.1%\n",
      "Test Error: \n",
      "Accuracy: 27.7%, Avg loss: 2.120178 \n",
      "\n",
      "batch 5, loss: 2.125633, accuracy: 32.8%\n",
      "Test Error: \n",
      "Accuracy: 28.0%, Avg loss: 2.113944 \n",
      "\n",
      "batch 6, loss: 2.149934, accuracy: 23.4%\n",
      "Test Error: \n",
      "Accuracy: 28.5%, Avg loss: 2.104420 \n",
      "\n",
      "batch 7, loss: 1.993043, accuracy: 32.8%\n",
      "Test Error: \n",
      "Accuracy: 28.6%, Avg loss: 2.098863 \n",
      "\n",
      "batch 8, loss: 2.009071, accuracy: 37.5%\n",
      "Test Error: \n",
      "Accuracy: 29.4%, Avg loss: 2.096516 \n",
      "\n",
      "batch 9, loss: 2.113937, accuracy: 26.6%\n",
      "Test Error: \n",
      "Accuracy: 29.3%, Avg loss: 2.089928 \n",
      "\n",
      "batch 10, loss: 2.023803, accuracy: 37.5%\n",
      "Test Error: \n",
      "Accuracy: 29.3%, Avg loss: 2.084436 \n",
      "\n",
      "batch 11, loss: 2.028102, accuracy: 32.8%\n",
      "Test Error: \n",
      "Accuracy: 29.2%, Avg loss: 2.076151 \n",
      "\n",
      "batch 12, loss: 2.064854, accuracy: 28.1%\n",
      "Test Error: \n",
      "Accuracy: 29.5%, Avg loss: 2.069382 \n",
      "\n",
      "batch 13, loss: 2.021093, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 30.0%, Avg loss: 2.063431 \n",
      "\n",
      "batch 14, loss: 2.054005, accuracy: 32.8%\n",
      "Test Error: \n",
      "Accuracy: 30.6%, Avg loss: 2.058680 \n",
      "\n",
      "batch 15, loss: 2.055004, accuracy: 28.1%\n",
      "Test Error: \n",
      "Accuracy: 30.6%, Avg loss: 2.052559 \n",
      "\n",
      "batch 16, loss: 2.047879, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 30.6%, Avg loss: 2.047090 \n",
      "\n",
      "batch 17, loss: 2.072781, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 30.9%, Avg loss: 2.041529 \n",
      "\n",
      "batch 18, loss: 1.911076, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 30.9%, Avg loss: 2.036933 \n",
      "\n",
      "batch 19, loss: 2.002507, accuracy: 29.7%\n",
      "Test Error: \n",
      "Accuracy: 31.6%, Avg loss: 2.031301 \n",
      "\n",
      "batch 20, loss: 1.997549, accuracy: 39.1%\n",
      "Test Error: \n",
      "Accuracy: 31.9%, Avg loss: 2.024372 \n",
      "\n",
      "batch 21, loss: 1.982065, accuracy: 34.4%\n",
      "Test Error: \n",
      "Accuracy: 32.2%, Avg loss: 2.016856 \n",
      "\n",
      "batch 22, loss: 1.942831, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 32.9%, Avg loss: 2.010417 \n",
      "\n",
      "batch 23, loss: 2.024863, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 32.6%, Avg loss: 2.007900 \n",
      "\n",
      "batch 24, loss: 1.947655, accuracy: 34.4%\n",
      "Test Error: \n",
      "Accuracy: 32.1%, Avg loss: 2.005171 \n",
      "\n",
      "batch 25, loss: 2.041466, accuracy: 28.1%\n",
      "Test Error: \n",
      "Accuracy: 32.4%, Avg loss: 2.000074 \n",
      "\n",
      "batch 26, loss: 1.954020, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 32.4%, Avg loss: 1.993097 \n",
      "\n",
      "batch 27, loss: 1.940298, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 32.8%, Avg loss: 1.988562 \n",
      "\n",
      "batch 28, loss: 1.996137, accuracy: 39.1%\n",
      "Test Error: \n",
      "Accuracy: 32.8%, Avg loss: 1.980004 \n",
      "\n",
      "batch 29, loss: 1.972416, accuracy: 34.4%\n",
      "Test Error: \n",
      "Accuracy: 33.7%, Avg loss: 1.975848 \n",
      "\n",
      "batch 30, loss: 1.974921, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 33.7%, Avg loss: 1.972444 \n",
      "\n",
      "batch 31, loss: 1.812673, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 34.2%, Avg loss: 1.967980 \n",
      "\n",
      "batch 32, loss: 1.976514, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 34.4%, Avg loss: 1.965044 \n",
      "\n",
      "batch 33, loss: 1.909599, accuracy: 37.5%\n",
      "Test Error: \n",
      "Accuracy: 34.4%, Avg loss: 1.963539 \n",
      "\n",
      "batch 34, loss: 1.949515, accuracy: 32.8%\n",
      "Test Error: \n",
      "Accuracy: 34.9%, Avg loss: 1.960293 \n",
      "\n",
      "batch 35, loss: 1.952689, accuracy: 32.8%\n",
      "Test Error: \n",
      "Accuracy: 34.9%, Avg loss: 1.956898 \n",
      "\n",
      "batch 36, loss: 1.989991, accuracy: 26.6%\n",
      "Test Error: \n",
      "Accuracy: 35.0%, Avg loss: 1.953985 \n",
      "\n",
      "batch 37, loss: 1.915334, accuracy: 39.1%\n",
      "Test Error: \n",
      "Accuracy: 35.1%, Avg loss: 1.952581 \n",
      "\n",
      "batch 38, loss: 1.867064, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 34.9%, Avg loss: 1.949777 \n",
      "\n",
      "batch 39, loss: 2.129259, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 35.0%, Avg loss: 1.946210 \n",
      "\n",
      "batch 40, loss: 1.881399, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 35.9%, Avg loss: 1.941363 \n",
      "\n",
      "batch 41, loss: 1.927393, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 35.9%, Avg loss: 1.933990 \n",
      "\n",
      "batch 42, loss: 1.928137, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 36.4%, Avg loss: 1.923975 \n",
      "\n",
      "batch 43, loss: 1.831870, accuracy: 42.2%\n",
      "Test Error: \n",
      "Accuracy: 36.2%, Avg loss: 1.916104 \n",
      "\n",
      "batch 44, loss: 1.905930, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 36.4%, Avg loss: 1.907358 \n",
      "\n",
      "batch 45, loss: 1.735068, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 37.0%, Avg loss: 1.898688 \n",
      "\n",
      "batch 46, loss: 1.882880, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 37.0%, Avg loss: 1.894365 \n",
      "\n",
      "batch 47, loss: 1.738746, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 37.5%, Avg loss: 1.890997 \n",
      "\n",
      "batch 48, loss: 1.741383, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 37.1%, Avg loss: 1.889829 \n",
      "\n",
      "batch 49, loss: 2.070099, accuracy: 29.7%\n",
      "Test Error: \n",
      "Accuracy: 38.2%, Avg loss: 1.878380 \n",
      "\n",
      "batch 50, loss: 1.842631, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 38.9%, Avg loss: 1.869617 \n",
      "\n",
      "batch 51, loss: 1.865182, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 38.9%, Avg loss: 1.866894 \n",
      "\n",
      "batch 52, loss: 1.852788, accuracy: 39.1%\n",
      "Test Error: \n",
      "Accuracy: 38.6%, Avg loss: 1.864826 \n",
      "\n",
      "batch 53, loss: 1.753394, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 39.2%, Avg loss: 1.861238 \n",
      "\n",
      "batch 54, loss: 1.825131, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 39.4%, Avg loss: 1.855646 \n",
      "\n",
      "batch 55, loss: 2.108324, accuracy: 26.6%\n",
      "Test Error: \n",
      "Accuracy: 39.4%, Avg loss: 1.849602 \n",
      "\n",
      "batch 56, loss: 2.115530, accuracy: 23.4%\n",
      "Test Error: \n",
      "Accuracy: 39.4%, Avg loss: 1.844659 \n",
      "\n",
      "batch 57, loss: 1.837626, accuracy: 34.4%\n",
      "Test Error: \n",
      "Accuracy: 39.5%, Avg loss: 1.839963 \n",
      "\n",
      "batch 58, loss: 2.008892, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 40.5%, Avg loss: 1.832159 \n",
      "\n",
      "batch 59, loss: 1.930017, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 41.1%, Avg loss: 1.827888 \n",
      "\n",
      "batch 60, loss: 1.748461, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 41.3%, Avg loss: 1.823962 \n",
      "\n",
      "batch 61, loss: 1.856820, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 41.1%, Avg loss: 1.822195 \n",
      "\n",
      "batch 62, loss: 1.840014, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 41.2%, Avg loss: 1.818612 \n",
      "\n",
      "batch 63, loss: 1.831292, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 41.1%, Avg loss: 1.817172 \n",
      "\n",
      "batch 64, loss: 2.003108, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 40.7%, Avg loss: 1.817876 \n",
      "\n",
      "batch 65, loss: 1.829581, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 40.5%, Avg loss: 1.813084 \n",
      "\n",
      "batch 66, loss: 1.858899, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 41.1%, Avg loss: 1.807505 \n",
      "\n",
      "batch 67, loss: 1.849258, accuracy: 37.5%\n",
      "Test Error: \n",
      "Accuracy: 41.6%, Avg loss: 1.803946 \n",
      "\n",
      "batch 68, loss: 1.818567, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 41.3%, Avg loss: 1.798379 \n",
      "\n",
      "batch 69, loss: 1.783576, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 41.8%, Avg loss: 1.792565 \n",
      "\n",
      "batch 70, loss: 1.663682, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 42.4%, Avg loss: 1.783149 \n",
      "\n",
      "batch 71, loss: 1.785527, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 42.3%, Avg loss: 1.779013 \n",
      "\n",
      "batch 72, loss: 1.570397, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 42.4%, Avg loss: 1.776329 \n",
      "\n",
      "batch 73, loss: 1.899151, accuracy: 39.1%\n",
      "Test Error: \n",
      "Accuracy: 42.9%, Avg loss: 1.769870 \n",
      "\n",
      "batch 74, loss: 1.809019, accuracy: 42.2%\n",
      "Test Error: \n",
      "Accuracy: 42.6%, Avg loss: 1.763289 \n",
      "\n",
      "batch 75, loss: 1.957226, accuracy: 37.5%\n",
      "Test Error: \n",
      "Accuracy: 42.3%, Avg loss: 1.764004 \n",
      "\n",
      "batch 76, loss: 1.808678, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 42.4%, Avg loss: 1.761389 \n",
      "\n",
      "batch 77, loss: 1.756723, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 42.7%, Avg loss: 1.755580 \n",
      "\n",
      "batch 78, loss: 1.841524, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 43.1%, Avg loss: 1.752202 \n",
      "\n",
      "batch 79, loss: 1.837570, accuracy: 32.8%\n",
      "Test Error: \n",
      "Accuracy: 43.0%, Avg loss: 1.749741 \n",
      "\n",
      "batch 80, loss: 1.625481, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 43.6%, Avg loss: 1.748658 \n",
      "\n",
      "batch 81, loss: 1.750718, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 43.5%, Avg loss: 1.748164 \n",
      "\n",
      "batch 82, loss: 1.787737, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 43.7%, Avg loss: 1.744849 \n",
      "\n",
      "batch 83, loss: 1.728800, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 44.0%, Avg loss: 1.738686 \n",
      "\n",
      "batch 84, loss: 1.769272, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 44.1%, Avg loss: 1.733444 \n",
      "\n",
      "batch 85, loss: 1.802990, accuracy: 39.1%\n",
      "Test Error: \n",
      "Accuracy: 44.3%, Avg loss: 1.727816 \n",
      "\n",
      "batch 86, loss: 1.892045, accuracy: 32.8%\n",
      "Test Error: \n",
      "Accuracy: 44.5%, Avg loss: 1.723171 \n",
      "\n",
      "batch 87, loss: 1.917136, accuracy: 35.9%\n",
      "Test Error: \n",
      "Accuracy: 44.8%, Avg loss: 1.720005 \n",
      "\n",
      "batch 88, loss: 1.683852, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 44.5%, Avg loss: 1.721164 \n",
      "\n",
      "batch 89, loss: 1.468903, accuracy: 57.8%\n",
      "Test Error: \n",
      "Accuracy: 44.4%, Avg loss: 1.717279 \n",
      "\n",
      "batch 90, loss: 1.679280, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 44.4%, Avg loss: 1.715187 \n",
      "\n",
      "batch 91, loss: 1.748144, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 44.3%, Avg loss: 1.712044 \n",
      "\n",
      "batch 92, loss: 1.710487, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 44.5%, Avg loss: 1.710865 \n",
      "\n",
      "batch 93, loss: 1.636881, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 45.1%, Avg loss: 1.707310 \n",
      "\n",
      "batch 94, loss: 1.669460, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 45.1%, Avg loss: 1.704967 \n",
      "\n",
      "batch 95, loss: 1.664135, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 45.1%, Avg loss: 1.704365 \n",
      "\n",
      "batch 96, loss: 1.514428, accuracy: 57.8%\n",
      "Test Error: \n",
      "Accuracy: 44.7%, Avg loss: 1.702449 \n",
      "\n",
      "batch 97, loss: 1.516692, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 44.9%, Avg loss: 1.700630 \n",
      "\n",
      "batch 98, loss: 1.787883, accuracy: 39.1%\n",
      "Test Error: \n",
      "Accuracy: 44.4%, Avg loss: 1.698870 \n",
      "\n",
      "batch 99, loss: 1.602682, accuracy: 37.5%\n",
      "Test Error: \n",
      "Accuracy: 44.7%, Avg loss: 1.692770 \n",
      "\n",
      "batch 100, loss: 1.520485, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 45.0%, Avg loss: 1.693181 \n",
      "\n",
      "batch 101, loss: 1.579381, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 44.8%, Avg loss: 1.692850 \n",
      "\n",
      "batch 102, loss: 1.760005, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 44.8%, Avg loss: 1.683970 \n",
      "\n",
      "batch 103, loss: 1.834517, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 44.8%, Avg loss: 1.681342 \n",
      "\n",
      "batch 104, loss: 1.501002, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 44.6%, Avg loss: 1.675137 \n",
      "\n",
      "batch 105, loss: 1.806525, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 44.5%, Avg loss: 1.671917 \n",
      "\n",
      "batch 106, loss: 1.545192, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 44.5%, Avg loss: 1.667242 \n",
      "\n",
      "batch 107, loss: 1.496793, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 44.9%, Avg loss: 1.665919 \n",
      "\n",
      "batch 108, loss: 1.501838, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 44.6%, Avg loss: 1.661164 \n",
      "\n",
      "batch 109, loss: 1.731470, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 44.8%, Avg loss: 1.657663 \n",
      "\n",
      "batch 110, loss: 1.710602, accuracy: 42.2%\n",
      "Test Error: \n",
      "Accuracy: 45.2%, Avg loss: 1.649351 \n",
      "\n",
      "batch 111, loss: 1.598986, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 45.1%, Avg loss: 1.648170 \n",
      "\n",
      "batch 112, loss: 1.599114, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 45.4%, Avg loss: 1.644089 \n",
      "\n",
      "batch 113, loss: 1.569027, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 46.7%, Avg loss: 1.635620 \n",
      "\n",
      "batch 114, loss: 1.429594, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 47.0%, Avg loss: 1.628438 \n",
      "\n",
      "batch 115, loss: 1.494543, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 47.2%, Avg loss: 1.624182 \n",
      "\n",
      "batch 116, loss: 1.432629, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 46.8%, Avg loss: 1.622507 \n",
      "\n",
      "batch 117, loss: 1.665354, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 46.9%, Avg loss: 1.618337 \n",
      "\n",
      "batch 118, loss: 1.550538, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 47.5%, Avg loss: 1.615192 \n",
      "\n",
      "batch 119, loss: 1.452544, accuracy: 59.4%\n",
      "Test Error: \n",
      "Accuracy: 47.4%, Avg loss: 1.608420 \n",
      "\n",
      "batch 120, loss: 1.574248, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 47.2%, Avg loss: 1.604883 \n",
      "\n",
      "batch 121, loss: 1.407091, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 48.0%, Avg loss: 1.599136 \n",
      "\n",
      "batch 122, loss: 1.470178, accuracy: 59.4%\n",
      "Test Error: \n",
      "Accuracy: 48.1%, Avg loss: 1.595373 \n",
      "\n",
      "batch 123, loss: 1.572809, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 47.7%, Avg loss: 1.590169 \n",
      "\n",
      "batch 124, loss: 1.539360, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 48.1%, Avg loss: 1.584635 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá</td></tr><tr><td>train_average_neuron_spikes</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà</td></tr><tr><td>train_loss</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>train_spike_percentage</td><td>‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>val_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_average_neuron_spikes</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_spike_percentage</td><td>‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.5</td></tr><tr><td>train_average_neuron_spikes</td><td>1.30938</td></tr><tr><td>train_loss</td><td>1.53936</td></tr><tr><td>train_spike_percentage</td><td>0.65625</td></tr><tr><td>val_acc</td><td>0.481</td></tr><tr><td>val_average_neuron_spikes</td><td>1.24755</td></tr><tr><td>val_loss</td><td>1.58464</td></tr><tr><td>val_spike_percentage</td><td>0.6527</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-1</strong> at: <a href='https://wandb.ai/yixing/ES-Randman10/runs/6ri90zn0' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/runs/6ri90zn0</a><br> View project at: <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10</a><br>Synced 5 W&B file(s), 0 media file(s), 500 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250617_191102-6ri90zn0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b518o35p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearn_beta: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_fn: cross-entropy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmirror: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_data_samples: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_hidden: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_input: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_model_samples: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_output: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_steps: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tregularization: none\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstd: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wyx/darwin_neuron/wandb/run-20250617_194005-b518o35p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yixing/ES-Randman10/runs/b518o35p' target=\"_blank\">zany-sweep-2</a></strong> to <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yixing/ES-Randman10/runs/b518o35p' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/runs/b518o35p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "batch 0, loss: 2.325347, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 10.0%, Avg loss: 2.325980 \n",
      "\n",
      "batch 1, loss: 2.314633, accuracy: 3.1%\n",
      "Test Error: \n",
      "Accuracy: 11.2%, Avg loss: 2.318789 \n",
      "\n",
      "batch 2, loss: 2.300352, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 11.7%, Avg loss: 2.313492 \n",
      "\n",
      "batch 3, loss: 2.340903, accuracy: 4.7%\n",
      "Test Error: \n",
      "Accuracy: 11.7%, Avg loss: 2.311439 \n",
      "\n",
      "batch 4, loss: 2.365021, accuracy: 4.7%\n",
      "Test Error: \n",
      "Accuracy: 11.2%, Avg loss: 2.310555 \n",
      "\n",
      "batch 5, loss: 2.308555, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.308208 \n",
      "\n",
      "batch 6, loss: 2.296499, accuracy: 4.7%\n",
      "Test Error: \n",
      "Accuracy: 11.3%, Avg loss: 2.307785 \n",
      "\n",
      "batch 7, loss: 2.333830, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 10.9%, Avg loss: 2.308995 \n",
      "\n",
      "batch 8, loss: 2.311252, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.1%, Avg loss: 2.307350 \n",
      "\n",
      "batch 9, loss: 2.307514, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 10.7%, Avg loss: 2.305935 \n",
      "\n",
      "batch 10, loss: 2.307033, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.1%, Avg loss: 2.304057 \n",
      "\n",
      "batch 11, loss: 2.320693, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 11.2%, Avg loss: 2.302025 \n",
      "\n",
      "batch 12, loss: 2.358156, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 11.2%, Avg loss: 2.300380 \n",
      "\n",
      "batch 13, loss: 2.313413, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 11.2%, Avg loss: 2.299045 \n",
      "\n",
      "batch 14, loss: 2.312574, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.3%, Avg loss: 2.298316 \n",
      "\n",
      "batch 15, loss: 2.320451, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 11.5%, Avg loss: 2.296895 \n",
      "\n",
      "batch 16, loss: 2.286839, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 11.3%, Avg loss: 2.297851 \n",
      "\n",
      "batch 17, loss: 2.301832, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 11.1%, Avg loss: 2.298679 \n",
      "\n",
      "batch 18, loss: 2.303783, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 11.1%, Avg loss: 2.298313 \n",
      "\n",
      "batch 19, loss: 2.306703, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.296832 \n",
      "\n",
      "batch 20, loss: 2.310223, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 11.5%, Avg loss: 2.296783 \n",
      "\n",
      "batch 21, loss: 2.311847, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.296081 \n",
      "\n",
      "batch 22, loss: 2.295431, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 11.4%, Avg loss: 2.296330 \n",
      "\n",
      "batch 23, loss: 2.316634, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 11.2%, Avg loss: 2.296553 \n",
      "\n",
      "batch 24, loss: 2.320918, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.5%, Avg loss: 2.295447 \n",
      "\n",
      "batch 25, loss: 2.296054, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 11.4%, Avg loss: 2.295757 \n",
      "\n",
      "batch 26, loss: 2.308157, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.295611 \n",
      "\n",
      "batch 27, loss: 2.285613, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.295068 \n",
      "\n",
      "batch 28, loss: 2.294961, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 11.7%, Avg loss: 2.293839 \n",
      "\n",
      "batch 29, loss: 2.292086, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 11.7%, Avg loss: 2.293185 \n",
      "\n",
      "batch 30, loss: 2.304337, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 11.7%, Avg loss: 2.293423 \n",
      "\n",
      "batch 31, loss: 2.316710, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.293078 \n",
      "\n",
      "batch 32, loss: 2.307715, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.293637 \n",
      "\n",
      "batch 33, loss: 2.316667, accuracy: 4.7%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.293763 \n",
      "\n",
      "batch 34, loss: 2.290317, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.292408 \n",
      "\n",
      "batch 35, loss: 2.307105, accuracy: 4.7%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.292465 \n",
      "\n",
      "batch 36, loss: 2.314071, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.292783 \n",
      "\n",
      "batch 37, loss: 2.305584, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.292453 \n",
      "\n",
      "batch 38, loss: 2.313211, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.291997 \n",
      "\n",
      "batch 39, loss: 2.285148, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 12.5%, Avg loss: 2.291224 \n",
      "\n",
      "batch 40, loss: 2.293256, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 12.4%, Avg loss: 2.292210 \n",
      "\n",
      "batch 41, loss: 2.280274, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 12.3%, Avg loss: 2.292387 \n",
      "\n",
      "batch 42, loss: 2.306408, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.292615 \n",
      "\n",
      "batch 43, loss: 2.299014, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.292329 \n",
      "\n",
      "batch 44, loss: 2.300302, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.292738 \n",
      "\n",
      "batch 45, loss: 2.286205, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.292546 \n",
      "\n",
      "batch 46, loss: 2.289730, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.292445 \n",
      "\n",
      "batch 47, loss: 2.294008, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.292254 \n",
      "\n",
      "batch 48, loss: 2.298674, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.292554 \n",
      "\n",
      "batch 49, loss: 2.275209, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.292666 \n",
      "\n",
      "batch 50, loss: 2.303385, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.291323 \n",
      "\n",
      "batch 51, loss: 2.300947, accuracy: 3.1%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.291091 \n",
      "\n",
      "batch 52, loss: 2.290134, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.290777 \n",
      "\n",
      "batch 53, loss: 2.299907, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.290794 \n",
      "\n",
      "batch 54, loss: 2.317020, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.290603 \n",
      "\n",
      "batch 55, loss: 2.293383, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.290421 \n",
      "\n",
      "batch 56, loss: 2.291289, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.289913 \n",
      "\n",
      "batch 57, loss: 2.291211, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.289355 \n",
      "\n",
      "batch 58, loss: 2.297868, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.289154 \n",
      "\n",
      "batch 59, loss: 2.289196, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.288821 \n",
      "\n",
      "batch 60, loss: 2.286557, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.288122 \n",
      "\n",
      "batch 61, loss: 2.292250, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.287717 \n",
      "\n",
      "batch 62, loss: 2.297485, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.287924 \n",
      "\n",
      "batch 63, loss: 2.303824, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.288558 \n",
      "\n",
      "batch 64, loss: 2.288837, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.289079 \n",
      "\n",
      "batch 65, loss: 2.276943, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.288958 \n",
      "\n",
      "batch 66, loss: 2.304100, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 11.7%, Avg loss: 2.288741 \n",
      "\n",
      "batch 67, loss: 2.311402, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 11.7%, Avg loss: 2.288517 \n",
      "\n",
      "batch 68, loss: 2.306981, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.288587 \n",
      "\n",
      "batch 69, loss: 2.302348, accuracy: 3.1%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.287995 \n",
      "\n",
      "batch 70, loss: 2.282621, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.287370 \n",
      "\n",
      "batch 71, loss: 2.323164, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.287456 \n",
      "\n",
      "batch 72, loss: 2.278873, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.287062 \n",
      "\n",
      "batch 73, loss: 2.274635, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 11.5%, Avg loss: 2.287001 \n",
      "\n",
      "batch 74, loss: 2.302898, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.7%, Avg loss: 2.286668 \n",
      "\n",
      "batch 75, loss: 2.285664, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.285687 \n",
      "\n",
      "batch 76, loss: 2.267085, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.285778 \n",
      "\n",
      "batch 77, loss: 2.298538, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.285434 \n",
      "\n",
      "batch 78, loss: 2.276230, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.284638 \n",
      "\n",
      "batch 79, loss: 2.290563, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.284939 \n",
      "\n",
      "batch 80, loss: 2.289365, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.283736 \n",
      "\n",
      "batch 81, loss: 2.305552, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.283531 \n",
      "\n",
      "batch 82, loss: 2.295370, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.283889 \n",
      "\n",
      "batch 83, loss: 2.322594, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 11.9%, Avg loss: 2.283830 \n",
      "\n",
      "batch 84, loss: 2.330524, accuracy: 4.7%\n",
      "Test Error: \n",
      "Accuracy: 12.1%, Avg loss: 2.283907 \n",
      "\n",
      "batch 85, loss: 2.283799, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.283821 \n",
      "\n",
      "batch 86, loss: 2.297506, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.284299 \n",
      "\n",
      "batch 87, loss: 2.278264, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 12.3%, Avg loss: 2.283788 \n",
      "\n",
      "batch 88, loss: 2.270592, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 12.3%, Avg loss: 2.284033 \n",
      "\n",
      "batch 89, loss: 2.287284, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 12.4%, Avg loss: 2.284232 \n",
      "\n",
      "batch 90, loss: 2.294245, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 12.4%, Avg loss: 2.284838 \n",
      "\n",
      "batch 91, loss: 2.270550, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 12.5%, Avg loss: 2.284776 \n",
      "\n",
      "batch 92, loss: 2.284562, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 12.4%, Avg loss: 2.284288 \n",
      "\n",
      "batch 93, loss: 2.292652, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 12.6%, Avg loss: 2.283463 \n",
      "\n",
      "batch 94, loss: 2.279278, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 12.6%, Avg loss: 2.282986 \n",
      "\n",
      "batch 95, loss: 2.288396, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 12.6%, Avg loss: 2.282974 \n",
      "\n",
      "batch 96, loss: 2.275305, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 12.3%, Avg loss: 2.283387 \n",
      "\n",
      "batch 97, loss: 2.263503, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 12.4%, Avg loss: 2.283194 \n",
      "\n",
      "batch 98, loss: 2.288210, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 12.4%, Avg loss: 2.282920 \n",
      "\n",
      "batch 99, loss: 2.300940, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 12.4%, Avg loss: 2.282182 \n",
      "\n",
      "batch 100, loss: 2.299817, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 12.5%, Avg loss: 2.281980 \n",
      "\n",
      "batch 101, loss: 2.294951, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 12.4%, Avg loss: 2.282003 \n",
      "\n",
      "batch 102, loss: 2.301809, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 12.8%, Avg loss: 2.281436 \n",
      "\n",
      "batch 103, loss: 2.271991, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 13.0%, Avg loss: 2.280646 \n",
      "\n",
      "batch 104, loss: 2.292286, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 12.9%, Avg loss: 2.280550 \n",
      "\n",
      "batch 105, loss: 2.287705, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 12.8%, Avg loss: 2.280765 \n",
      "\n",
      "batch 106, loss: 2.303860, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 12.7%, Avg loss: 2.281003 \n",
      "\n",
      "batch 107, loss: 2.268456, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 12.6%, Avg loss: 2.280607 \n",
      "\n",
      "batch 108, loss: 2.306385, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 12.3%, Avg loss: 2.281086 \n",
      "\n",
      "batch 109, loss: 2.309536, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 12.4%, Avg loss: 2.280485 \n",
      "\n",
      "batch 110, loss: 2.295469, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 12.3%, Avg loss: 2.280567 \n",
      "\n",
      "batch 111, loss: 2.276322, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.281008 \n",
      "\n",
      "batch 112, loss: 2.260104, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.280615 \n",
      "\n",
      "batch 113, loss: 2.281204, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.280740 \n",
      "\n",
      "batch 114, loss: 2.278722, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.281041 \n",
      "\n",
      "batch 115, loss: 2.289553, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 12.1%, Avg loss: 2.281986 \n",
      "\n",
      "batch 116, loss: 2.265813, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.282435 \n",
      "\n",
      "batch 117, loss: 2.304686, accuracy: 4.7%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.282200 \n",
      "\n",
      "batch 118, loss: 2.295296, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.282196 \n",
      "\n",
      "batch 119, loss: 2.293542, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 12.4%, Avg loss: 2.281677 \n",
      "\n",
      "batch 120, loss: 2.295033, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 12.6%, Avg loss: 2.281696 \n",
      "\n",
      "batch 121, loss: 2.292045, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 12.5%, Avg loss: 2.282693 \n",
      "\n",
      "batch 122, loss: 2.271837, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 12.6%, Avg loss: 2.282298 \n",
      "\n",
      "batch 123, loss: 2.297259, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 12.8%, Avg loss: 2.281024 \n",
      "\n",
      "batch 124, loss: 2.284081, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.280606 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "batch 0, loss: 2.273861, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.279951 \n",
      "\n",
      "batch 1, loss: 2.268672, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 13.4%, Avg loss: 2.279380 \n",
      "\n",
      "batch 2, loss: 2.280894, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 13.6%, Avg loss: 2.279032 \n",
      "\n",
      "batch 3, loss: 2.248926, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 13.8%, Avg loss: 2.277768 \n",
      "\n",
      "batch 4, loss: 2.276968, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 13.9%, Avg loss: 2.277315 \n",
      "\n",
      "batch 5, loss: 2.296471, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 14.0%, Avg loss: 2.276320 \n",
      "\n",
      "batch 6, loss: 2.296959, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 13.9%, Avg loss: 2.277267 \n",
      "\n",
      "batch 7, loss: 2.283601, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 14.1%, Avg loss: 2.275722 \n",
      "\n",
      "batch 8, loss: 2.283795, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 14.3%, Avg loss: 2.274492 \n",
      "\n",
      "batch 9, loss: 2.297567, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 14.2%, Avg loss: 2.274806 \n",
      "\n",
      "batch 10, loss: 2.268899, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 14.3%, Avg loss: 2.274294 \n",
      "\n",
      "batch 11, loss: 2.268177, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 14.3%, Avg loss: 2.273329 \n",
      "\n",
      "batch 12, loss: 2.250479, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 14.5%, Avg loss: 2.273704 \n",
      "\n",
      "batch 13, loss: 2.294394, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 14.7%, Avg loss: 2.272999 \n",
      "\n",
      "batch 14, loss: 2.277396, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 14.8%, Avg loss: 2.272629 \n",
      "\n",
      "batch 15, loss: 2.254563, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 14.9%, Avg loss: 2.271988 \n",
      "\n",
      "batch 16, loss: 2.268805, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 15.1%, Avg loss: 2.271641 \n",
      "\n",
      "batch 17, loss: 2.284487, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 15.0%, Avg loss: 2.271072 \n",
      "\n",
      "batch 18, loss: 2.258910, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 14.9%, Avg loss: 2.271060 \n",
      "\n",
      "batch 19, loss: 2.267739, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 15.1%, Avg loss: 2.270219 \n",
      "\n",
      "batch 20, loss: 2.254472, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 15.0%, Avg loss: 2.270202 \n",
      "\n",
      "batch 21, loss: 2.291890, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 14.8%, Avg loss: 2.270339 \n",
      "\n",
      "batch 22, loss: 2.277585, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 15.2%, Avg loss: 2.269762 \n",
      "\n",
      "batch 23, loss: 2.270566, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 15.3%, Avg loss: 2.269184 \n",
      "\n",
      "batch 24, loss: 2.271178, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 15.4%, Avg loss: 2.268449 \n",
      "\n",
      "batch 25, loss: 2.277559, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 15.4%, Avg loss: 2.268206 \n",
      "\n",
      "batch 26, loss: 2.273760, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 15.4%, Avg loss: 2.267808 \n",
      "\n",
      "batch 27, loss: 2.261859, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 15.3%, Avg loss: 2.268076 \n",
      "\n",
      "batch 28, loss: 2.273392, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 15.3%, Avg loss: 2.268804 \n",
      "\n",
      "batch 29, loss: 2.263745, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 15.3%, Avg loss: 2.269707 \n",
      "\n",
      "batch 30, loss: 2.286084, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 15.2%, Avg loss: 2.269837 \n",
      "\n",
      "batch 31, loss: 2.260215, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 15.3%, Avg loss: 2.270081 \n",
      "\n",
      "batch 32, loss: 2.289993, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 15.2%, Avg loss: 2.270557 \n",
      "\n",
      "batch 33, loss: 2.281793, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 15.3%, Avg loss: 2.270547 \n",
      "\n",
      "batch 34, loss: 2.284673, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 15.5%, Avg loss: 2.269682 \n",
      "\n",
      "batch 35, loss: 2.286322, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 15.9%, Avg loss: 2.269132 \n",
      "\n",
      "batch 36, loss: 2.273487, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 16.2%, Avg loss: 2.268439 \n",
      "\n",
      "batch 37, loss: 2.261547, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 16.0%, Avg loss: 2.268148 \n",
      "\n",
      "batch 38, loss: 2.290274, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 16.0%, Avg loss: 2.267658 \n",
      "\n",
      "batch 39, loss: 2.301194, accuracy: 4.7%\n",
      "Test Error: \n",
      "Accuracy: 15.8%, Avg loss: 2.266927 \n",
      "\n",
      "batch 40, loss: 2.264390, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 15.8%, Avg loss: 2.266451 \n",
      "\n",
      "batch 41, loss: 2.275159, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 15.8%, Avg loss: 2.266429 \n",
      "\n",
      "batch 42, loss: 2.265652, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 15.8%, Avg loss: 2.266108 \n",
      "\n",
      "batch 43, loss: 2.266447, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 16.0%, Avg loss: 2.265056 \n",
      "\n",
      "batch 44, loss: 2.251393, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 16.1%, Avg loss: 2.265235 \n",
      "\n",
      "batch 45, loss: 2.263460, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 16.0%, Avg loss: 2.265265 \n",
      "\n",
      "batch 46, loss: 2.296326, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 16.1%, Avg loss: 2.264174 \n",
      "\n",
      "batch 47, loss: 2.247642, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 16.0%, Avg loss: 2.263631 \n",
      "\n",
      "batch 48, loss: 2.249115, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 16.1%, Avg loss: 2.263261 \n",
      "\n",
      "batch 49, loss: 2.260278, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 16.2%, Avg loss: 2.262106 \n",
      "\n",
      "batch 50, loss: 2.274504, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 16.6%, Avg loss: 2.262227 \n",
      "\n",
      "batch 51, loss: 2.272140, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 16.9%, Avg loss: 2.261467 \n",
      "\n",
      "batch 52, loss: 2.297701, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 17.2%, Avg loss: 2.261602 \n",
      "\n",
      "batch 53, loss: 2.291017, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 17.3%, Avg loss: 2.261528 \n",
      "\n",
      "batch 54, loss: 2.250431, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 17.1%, Avg loss: 2.261289 \n",
      "\n",
      "batch 55, loss: 2.273513, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 17.0%, Avg loss: 2.261152 \n",
      "\n",
      "batch 56, loss: 2.287014, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 17.1%, Avg loss: 2.259949 \n",
      "\n",
      "batch 57, loss: 2.290323, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 17.0%, Avg loss: 2.260255 \n",
      "\n",
      "batch 58, loss: 2.257128, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 17.2%, Avg loss: 2.259567 \n",
      "\n",
      "batch 59, loss: 2.259274, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 17.1%, Avg loss: 2.259133 \n",
      "\n",
      "batch 60, loss: 2.256799, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 17.1%, Avg loss: 2.258705 \n",
      "\n",
      "batch 61, loss: 2.243548, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 17.0%, Avg loss: 2.258564 \n",
      "\n",
      "batch 62, loss: 2.246103, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 16.8%, Avg loss: 2.258123 \n",
      "\n",
      "batch 63, loss: 2.245771, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 16.4%, Avg loss: 2.259129 \n",
      "\n",
      "batch 64, loss: 2.260690, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 16.1%, Avg loss: 2.258970 \n",
      "\n",
      "batch 65, loss: 2.308804, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 15.0%, Avg loss: 2.258877 \n",
      "\n",
      "batch 66, loss: 2.287685, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 15.4%, Avg loss: 2.259170 \n",
      "\n",
      "batch 67, loss: 2.256928, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 15.5%, Avg loss: 2.259209 \n",
      "\n",
      "batch 68, loss: 2.309482, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 15.2%, Avg loss: 2.258920 \n",
      "\n",
      "batch 69, loss: 2.257845, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 15.5%, Avg loss: 2.258888 \n",
      "\n",
      "batch 70, loss: 2.281236, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 15.4%, Avg loss: 2.258373 \n",
      "\n",
      "batch 71, loss: 2.244749, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 15.6%, Avg loss: 2.257846 \n",
      "\n",
      "batch 72, loss: 2.271789, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 15.8%, Avg loss: 2.256321 \n",
      "\n",
      "batch 73, loss: 2.274644, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 15.5%, Avg loss: 2.255915 \n",
      "\n",
      "batch 74, loss: 2.267929, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 16.4%, Avg loss: 2.255710 \n",
      "\n",
      "batch 75, loss: 2.278168, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 16.4%, Avg loss: 2.254790 \n",
      "\n",
      "batch 76, loss: 2.252652, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 16.1%, Avg loss: 2.255378 \n",
      "\n",
      "batch 77, loss: 2.291588, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 16.2%, Avg loss: 2.254228 \n",
      "\n",
      "batch 78, loss: 2.280006, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 16.9%, Avg loss: 2.252523 \n",
      "\n",
      "batch 79, loss: 2.274426, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 16.9%, Avg loss: 2.251206 \n",
      "\n",
      "batch 80, loss: 2.268263, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 17.0%, Avg loss: 2.250834 \n",
      "\n",
      "batch 81, loss: 2.259870, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 16.7%, Avg loss: 2.251733 \n",
      "\n",
      "batch 82, loss: 2.294168, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 17.3%, Avg loss: 2.250162 \n",
      "\n",
      "batch 83, loss: 2.262981, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 17.6%, Avg loss: 2.249395 \n",
      "\n",
      "batch 84, loss: 2.265855, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 17.6%, Avg loss: 2.249431 \n",
      "\n",
      "batch 85, loss: 2.263855, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 17.6%, Avg loss: 2.249061 \n",
      "\n",
      "batch 86, loss: 2.232860, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 17.6%, Avg loss: 2.246639 \n",
      "\n",
      "batch 87, loss: 2.220732, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 17.8%, Avg loss: 2.244466 \n",
      "\n",
      "batch 88, loss: 2.263947, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 18.1%, Avg loss: 2.242073 \n",
      "\n",
      "batch 89, loss: 2.270288, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 17.9%, Avg loss: 2.241802 \n",
      "\n",
      "batch 90, loss: 2.283873, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 18.0%, Avg loss: 2.240100 \n",
      "\n",
      "batch 91, loss: 2.222607, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 18.1%, Avg loss: 2.239584 \n",
      "\n",
      "batch 92, loss: 2.269623, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 18.6%, Avg loss: 2.239067 \n",
      "\n",
      "batch 93, loss: 2.237146, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 19.0%, Avg loss: 2.238414 \n",
      "\n",
      "batch 94, loss: 2.278575, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 19.0%, Avg loss: 2.238062 \n",
      "\n",
      "batch 95, loss: 2.282598, accuracy: 28.1%\n",
      "Test Error: \n",
      "Accuracy: 19.6%, Avg loss: 2.236883 \n",
      "\n",
      "batch 96, loss: 2.213013, accuracy: 26.6%\n",
      "Test Error: \n",
      "Accuracy: 19.1%, Avg loss: 2.236412 \n",
      "\n",
      "batch 97, loss: 2.240715, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 19.4%, Avg loss: 2.235907 \n",
      "\n",
      "batch 98, loss: 2.240912, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 19.8%, Avg loss: 2.234572 \n",
      "\n",
      "batch 99, loss: 2.287750, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 20.1%, Avg loss: 2.234619 \n",
      "\n",
      "batch 100, loss: 2.262181, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 20.0%, Avg loss: 2.232503 \n",
      "\n",
      "batch 101, loss: 2.238961, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 20.1%, Avg loss: 2.230647 \n",
      "\n",
      "batch 102, loss: 2.228560, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 20.0%, Avg loss: 2.229789 \n",
      "\n",
      "batch 103, loss: 2.208555, accuracy: 23.4%\n",
      "Test Error: \n",
      "Accuracy: 20.5%, Avg loss: 2.227825 \n",
      "\n",
      "batch 104, loss: 2.232581, accuracy: 23.4%\n",
      "Test Error: \n",
      "Accuracy: 20.9%, Avg loss: 2.227421 \n",
      "\n",
      "batch 105, loss: 2.246398, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 20.6%, Avg loss: 2.226377 \n",
      "\n",
      "batch 106, loss: 2.254829, accuracy: 17.2%\n",
      "Test Error: \n",
      "Accuracy: 20.4%, Avg loss: 2.225457 \n",
      "\n",
      "batch 107, loss: 2.265295, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 20.2%, Avg loss: 2.224500 \n",
      "\n",
      "batch 108, loss: 2.199109, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 20.5%, Avg loss: 2.223726 \n",
      "\n",
      "batch 109, loss: 2.242965, accuracy: 23.4%\n",
      "Test Error: \n",
      "Accuracy: 20.0%, Avg loss: 2.226510 \n",
      "\n",
      "batch 110, loss: 2.202575, accuracy: 25.0%\n",
      "Test Error: \n",
      "Accuracy: 20.1%, Avg loss: 2.225110 \n",
      "\n",
      "batch 111, loss: 2.227920, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 20.1%, Avg loss: 2.223112 \n",
      "\n",
      "batch 112, loss: 2.239178, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 20.4%, Avg loss: 2.222254 \n",
      "\n",
      "batch 113, loss: 2.209668, accuracy: 26.6%\n",
      "Test Error: \n",
      "Accuracy: 20.1%, Avg loss: 2.222232 \n",
      "\n",
      "batch 114, loss: 2.191116, accuracy: 26.6%\n",
      "Test Error: \n",
      "Accuracy: 20.1%, Avg loss: 2.220802 \n",
      "\n",
      "batch 115, loss: 2.180811, accuracy: 23.4%\n",
      "Test Error: \n",
      "Accuracy: 20.1%, Avg loss: 2.217763 \n",
      "\n",
      "batch 116, loss: 2.190187, accuracy: 25.0%\n",
      "Test Error: \n",
      "Accuracy: 20.6%, Avg loss: 2.213723 \n",
      "\n",
      "batch 117, loss: 2.200624, accuracy: 23.4%\n",
      "Test Error: \n",
      "Accuracy: 20.4%, Avg loss: 2.209955 \n",
      "\n",
      "batch 118, loss: 2.172277, accuracy: 28.1%\n",
      "Test Error: \n",
      "Accuracy: 20.8%, Avg loss: 2.207550 \n",
      "\n",
      "batch 119, loss: 2.217000, accuracy: 18.8%\n",
      "Test Error: \n",
      "Accuracy: 21.3%, Avg loss: 2.205860 \n",
      "\n",
      "batch 120, loss: 2.214907, accuracy: 25.0%\n",
      "Test Error: \n",
      "Accuracy: 21.5%, Avg loss: 2.202633 \n",
      "\n",
      "batch 121, loss: 2.203683, accuracy: 21.9%\n",
      "Test Error: \n",
      "Accuracy: 21.8%, Avg loss: 2.199264 \n",
      "\n",
      "batch 122, loss: 2.204537, accuracy: 20.3%\n",
      "Test Error: \n",
      "Accuracy: 21.6%, Avg loss: 2.196306 \n",
      "\n",
      "batch 123, loss: 2.207548, accuracy: 15.6%\n",
      "Test Error: \n",
      "Accuracy: 21.7%, Avg loss: 2.192593 \n",
      "\n",
      "batch 124, loss: 2.165743, accuracy: 28.1%\n",
      "Test Error: \n",
      "Accuracy: 21.9%, Avg loss: 2.189147 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñà‚ñá</td></tr><tr><td>train_average_neuron_spikes</td><td>‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>train_loss</td><td>‚ñÜ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>train_spike_percentage</td><td>‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>val_acc</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_average_neuron_spikes</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_spike_percentage</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.28125</td></tr><tr><td>train_average_neuron_spikes</td><td>0.30625</td></tr><tr><td>train_loss</td><td>2.16574</td></tr><tr><td>train_spike_percentage</td><td>0.26094</td></tr><tr><td>val_acc</td><td>0.2185</td></tr><tr><td>val_average_neuron_spikes</td><td>0.3176</td></tr><tr><td>val_loss</td><td>2.18915</td></tr><tr><td>val_spike_percentage</td><td>0.27345</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-sweep-2</strong> at: <a href='https://wandb.ai/yixing/ES-Randman10/runs/b518o35p' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/runs/b518o35p</a><br> View project at: <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10</a><br>Synced 5 W&B file(s), 0 media file(s), 500 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250617_194005-b518o35p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u6apvm5l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearn_beta: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_fn: cross-entropy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmirror: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_data_samples: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_hidden: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_input: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_model_samples: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_output: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_steps: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tregularization: none\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstd: 0.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wyx/darwin_neuron/wandb/run-20250617_201124-u6apvm5l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yixing/ES-Randman10/runs/u6apvm5l' target=\"_blank\">wobbly-sweep-3</a></strong> to <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yixing/ES-Randman10/runs/u6apvm5l' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/runs/u6apvm5l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "batch 0, loss: 2.333093, accuracy: 12.1%\n",
      "Test Error: \n",
      "Accuracy: 10.0%, Avg loss: 2.353518 \n",
      "\n",
      "batch 1, loss: 2.312753, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 10.0%, Avg loss: 2.344738 \n",
      "\n",
      "batch 2, loss: 2.342576, accuracy: 8.2%\n",
      "Test Error: \n",
      "Accuracy: 9.8%, Avg loss: 2.335716 \n",
      "\n",
      "batch 3, loss: 2.305691, accuracy: 9.0%\n",
      "Test Error: \n",
      "Accuracy: 9.4%, Avg loss: 2.334679 \n",
      "\n",
      "batch 4, loss: 2.357495, accuracy: 7.4%\n",
      "Test Error: \n",
      "Accuracy: 9.2%, Avg loss: 2.333783 \n",
      "\n",
      "batch 5, loss: 2.334923, accuracy: 10.2%\n",
      "Test Error: \n",
      "Accuracy: 9.2%, Avg loss: 2.331923 \n",
      "\n",
      "batch 6, loss: 2.333478, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 9.3%, Avg loss: 2.328658 \n",
      "\n",
      "batch 7, loss: 2.322394, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 9.3%, Avg loss: 2.328222 \n",
      "\n",
      "batch 8, loss: 2.315960, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 9.7%, Avg loss: 2.325693 \n",
      "\n",
      "batch 9, loss: 2.337746, accuracy: 8.6%\n",
      "Test Error: \n",
      "Accuracy: 9.1%, Avg loss: 2.325589 \n",
      "\n",
      "batch 10, loss: 2.330233, accuracy: 7.4%\n",
      "Test Error: \n",
      "Accuracy: 8.5%, Avg loss: 2.325951 \n",
      "\n",
      "batch 11, loss: 2.293756, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 9.0%, Avg loss: 2.323982 \n",
      "\n",
      "batch 12, loss: 2.317088, accuracy: 10.2%\n",
      "Test Error: \n",
      "Accuracy: 8.9%, Avg loss: 2.323452 \n",
      "\n",
      "batch 13, loss: 2.326553, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 9.0%, Avg loss: 2.322143 \n",
      "\n",
      "batch 14, loss: 2.315858, accuracy: 11.3%\n",
      "Test Error: \n",
      "Accuracy: 9.6%, Avg loss: 2.322047 \n",
      "\n",
      "batch 15, loss: 2.329058, accuracy: 11.3%\n",
      "Test Error: \n",
      "Accuracy: 9.6%, Avg loss: 2.320314 \n",
      "\n",
      "batch 16, loss: 2.333082, accuracy: 11.3%\n",
      "Test Error: \n",
      "Accuracy: 10.0%, Avg loss: 2.318192 \n",
      "\n",
      "batch 17, loss: 2.314296, accuracy: 9.0%\n",
      "Test Error: \n",
      "Accuracy: 10.2%, Avg loss: 2.317621 \n",
      "\n",
      "batch 18, loss: 2.324282, accuracy: 11.7%\n",
      "Test Error: \n",
      "Accuracy: 10.0%, Avg loss: 2.316388 \n",
      "\n",
      "batch 19, loss: 2.319057, accuracy: 7.4%\n",
      "Test Error: \n",
      "Accuracy: 10.1%, Avg loss: 2.315270 \n",
      "\n",
      "batch 20, loss: 2.314641, accuracy: 8.6%\n",
      "Test Error: \n",
      "Accuracy: 9.8%, Avg loss: 2.314325 \n",
      "\n",
      "batch 21, loss: 2.312480, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 9.8%, Avg loss: 2.312425 \n",
      "\n",
      "batch 22, loss: 2.320217, accuracy: 10.5%\n",
      "Test Error: \n",
      "Accuracy: 10.2%, Avg loss: 2.310480 \n",
      "\n",
      "batch 23, loss: 2.314670, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 10.2%, Avg loss: 2.309330 \n",
      "\n",
      "batch 24, loss: 2.299901, accuracy: 13.7%\n",
      "Test Error: \n",
      "Accuracy: 10.4%, Avg loss: 2.309902 \n",
      "\n",
      "batch 25, loss: 2.312975, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 10.8%, Avg loss: 2.309190 \n",
      "\n",
      "batch 26, loss: 2.329655, accuracy: 8.6%\n",
      "Test Error: \n",
      "Accuracy: 10.6%, Avg loss: 2.306849 \n",
      "\n",
      "batch 27, loss: 2.321397, accuracy: 8.6%\n",
      "Test Error: \n",
      "Accuracy: 10.7%, Avg loss: 2.305324 \n",
      "\n",
      "batch 28, loss: 2.305129, accuracy: 11.7%\n",
      "Test Error: \n",
      "Accuracy: 11.0%, Avg loss: 2.303976 \n",
      "\n",
      "batch 29, loss: 2.316663, accuracy: 9.0%\n",
      "Test Error: \n",
      "Accuracy: 11.1%, Avg loss: 2.301920 \n",
      "\n",
      "batch 30, loss: 2.306105, accuracy: 9.0%\n",
      "Test Error: \n",
      "Accuracy: 10.9%, Avg loss: 2.301747 \n",
      "\n",
      "batch 31, loss: 2.318613, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 11.3%, Avg loss: 2.300753 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "batch 0, loss: 2.317233, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 11.5%, Avg loss: 2.299822 \n",
      "\n",
      "batch 1, loss: 2.292821, accuracy: 13.7%\n",
      "Test Error: \n",
      "Accuracy: 11.3%, Avg loss: 2.299479 \n",
      "\n",
      "batch 2, loss: 2.308129, accuracy: 10.5%\n",
      "Test Error: \n",
      "Accuracy: 11.5%, Avg loss: 2.297638 \n",
      "\n",
      "batch 3, loss: 2.301358, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 11.7%, Avg loss: 2.297036 \n",
      "\n",
      "batch 4, loss: 2.315463, accuracy: 7.0%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.297142 \n",
      "\n",
      "batch 5, loss: 2.302320, accuracy: 11.3%\n",
      "Test Error: \n",
      "Accuracy: 11.6%, Avg loss: 2.296828 \n",
      "\n",
      "batch 6, loss: 2.311742, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 11.7%, Avg loss: 2.296282 \n",
      "\n",
      "batch 7, loss: 2.288791, accuracy: 10.2%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.295570 \n",
      "\n",
      "batch 8, loss: 2.304184, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 11.8%, Avg loss: 2.295703 \n",
      "\n",
      "batch 9, loss: 2.302118, accuracy: 11.3%\n",
      "Test Error: \n",
      "Accuracy: 12.0%, Avg loss: 2.295311 \n",
      "\n",
      "batch 10, loss: 2.306379, accuracy: 8.6%\n",
      "Test Error: \n",
      "Accuracy: 12.1%, Avg loss: 2.294227 \n",
      "\n",
      "batch 11, loss: 2.313198, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.293446 \n",
      "\n",
      "batch 12, loss: 2.287900, accuracy: 12.9%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.293047 \n",
      "\n",
      "batch 13, loss: 2.306493, accuracy: 11.3%\n",
      "Test Error: \n",
      "Accuracy: 12.2%, Avg loss: 2.292501 \n",
      "\n",
      "batch 14, loss: 2.294325, accuracy: 11.7%\n",
      "Test Error: \n",
      "Accuracy: 12.1%, Avg loss: 2.292697 \n",
      "\n",
      "batch 15, loss: 2.293140, accuracy: 13.3%\n",
      "Test Error: \n",
      "Accuracy: 12.1%, Avg loss: 2.292251 \n",
      "\n",
      "batch 16, loss: 2.292755, accuracy: 12.9%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.291639 \n",
      "\n",
      "batch 17, loss: 2.281736, accuracy: 12.1%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.291017 \n",
      "\n",
      "batch 18, loss: 2.293907, accuracy: 12.1%\n",
      "Test Error: \n",
      "Accuracy: 13.1%, Avg loss: 2.290983 \n",
      "\n",
      "batch 19, loss: 2.294684, accuracy: 11.7%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.291214 \n",
      "\n",
      "batch 20, loss: 2.295647, accuracy: 10.5%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.290964 \n",
      "\n",
      "batch 21, loss: 2.278388, accuracy: 13.3%\n",
      "Test Error: \n",
      "Accuracy: 13.5%, Avg loss: 2.290466 \n",
      "\n",
      "batch 22, loss: 2.293474, accuracy: 12.1%\n",
      "Test Error: \n",
      "Accuracy: 13.4%, Avg loss: 2.290442 \n",
      "\n",
      "batch 23, loss: 2.280203, accuracy: 14.1%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.290797 \n",
      "\n",
      "batch 24, loss: 2.282000, accuracy: 16.0%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.291009 \n",
      "\n",
      "batch 25, loss: 2.295743, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.290741 \n",
      "\n",
      "batch 26, loss: 2.288415, accuracy: 10.2%\n",
      "Test Error: \n",
      "Accuracy: 13.1%, Avg loss: 2.291341 \n",
      "\n",
      "batch 27, loss: 2.298368, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.290351 \n",
      "\n",
      "batch 28, loss: 2.289035, accuracy: 11.3%\n",
      "Test Error: \n",
      "Accuracy: 13.2%, Avg loss: 2.290597 \n",
      "\n",
      "batch 29, loss: 2.290752, accuracy: 14.8%\n",
      "Test Error: \n",
      "Accuracy: 13.1%, Avg loss: 2.291451 \n",
      "\n",
      "batch 30, loss: 2.301311, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 13.0%, Avg loss: 2.291079 \n",
      "\n",
      "batch 31, loss: 2.293248, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 13.0%, Avg loss: 2.290594 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>‚ñÜ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÜ</td></tr><tr><td>train_average_neuron_spikes</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_loss</td><td>‚ñÜ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ</td></tr><tr><td>train_spike_percentage</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc</td><td>‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>val_average_neuron_spikes</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_spike_percentage</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.125</td></tr><tr><td>train_average_neuron_spikes</td><td>0.1</td></tr><tr><td>train_loss</td><td>2.29325</td></tr><tr><td>train_spike_percentage</td><td>0.1</td></tr><tr><td>val_acc</td><td>0.13</td></tr><tr><td>val_average_neuron_spikes</td><td>0.10375</td></tr><tr><td>val_loss</td><td>2.29059</td></tr><tr><td>val_spike_percentage</td><td>0.10165</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-sweep-3</strong> at: <a href='https://wandb.ai/yixing/ES-Randman10/runs/u6apvm5l' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/runs/u6apvm5l</a><br> View project at: <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10</a><br>Synced 5 W&B file(s), 0 media file(s), 128 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250617_201124-u6apvm5l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v2d3ock2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearn_beta: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_fn: cross-entropy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmirror: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_data_samples: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_hidden: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_input: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_model_samples: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_output: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnb_steps: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tregularization: none\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstd: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.3s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wyx/darwin_neuron/wandb/run-20250617_202820-v2d3ock2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yixing/ES-Randman10/runs/v2d3ock2' target=\"_blank\">unique-sweep-4</a></strong> to <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/sweeps/78qhh6zn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yixing/ES-Randman10/runs/v2d3ock2' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/runs/v2d3ock2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "batch 0, loss: 2.355347, accuracy: 7.0%\n",
      "Test Error: \n",
      "Accuracy: 8.2%, Avg loss: 2.353222 \n",
      "\n",
      "batch 1, loss: 2.343258, accuracy: 5.1%\n",
      "Test Error: \n",
      "Accuracy: 7.5%, Avg loss: 2.349075 \n",
      "\n",
      "batch 2, loss: 2.330523, accuracy: 5.5%\n",
      "Test Error: \n",
      "Accuracy: 6.5%, Avg loss: 2.347209 \n",
      "\n",
      "batch 3, loss: 2.347730, accuracy: 5.5%\n",
      "Test Error: \n",
      "Accuracy: 6.3%, Avg loss: 2.343578 \n",
      "\n",
      "batch 4, loss: 2.334744, accuracy: 8.6%\n",
      "Test Error: \n",
      "Accuracy: 6.2%, Avg loss: 2.340262 \n",
      "\n",
      "batch 5, loss: 2.323638, accuracy: 8.2%\n",
      "Test Error: \n",
      "Accuracy: 6.0%, Avg loss: 2.336810 \n",
      "\n",
      "batch 6, loss: 2.341033, accuracy: 5.1%\n",
      "Test Error: \n",
      "Accuracy: 6.2%, Avg loss: 2.334517 \n",
      "\n",
      "batch 7, loss: 2.341938, accuracy: 7.0%\n",
      "Test Error: \n",
      "Accuracy: 6.6%, Avg loss: 2.331636 \n",
      "\n",
      "batch 8, loss: 2.340763, accuracy: 10.2%\n",
      "Test Error: \n",
      "Accuracy: 6.8%, Avg loss: 2.330243 \n",
      "\n",
      "batch 9, loss: 2.333907, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 6.5%, Avg loss: 2.329560 \n",
      "\n",
      "batch 10, loss: 2.338249, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 6.8%, Avg loss: 2.327589 \n",
      "\n",
      "batch 11, loss: 2.331388, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 7.0%, Avg loss: 2.327050 \n",
      "\n",
      "batch 12, loss: 2.328571, accuracy: 6.2%\n",
      "Test Error: \n",
      "Accuracy: 7.2%, Avg loss: 2.324310 \n",
      "\n",
      "batch 13, loss: 2.309764, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 7.2%, Avg loss: 2.322524 \n",
      "\n",
      "batch 14, loss: 2.341100, accuracy: 5.1%\n",
      "Test Error: \n",
      "Accuracy: 7.1%, Avg loss: 2.322309 \n",
      "\n",
      "batch 15, loss: 2.328986, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 7.3%, Avg loss: 2.322097 \n",
      "\n",
      "batch 16, loss: 2.313226, accuracy: 9.0%\n",
      "Test Error: \n",
      "Accuracy: 7.4%, Avg loss: 2.320173 \n",
      "\n",
      "batch 17, loss: 2.325011, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 7.4%, Avg loss: 2.319899 \n",
      "\n",
      "batch 18, loss: 2.327508, accuracy: 6.6%\n",
      "Test Error: \n",
      "Accuracy: 7.4%, Avg loss: 2.318831 \n",
      "\n",
      "batch 19, loss: 2.334012, accuracy: 6.6%\n",
      "Test Error: \n",
      "Accuracy: 7.5%, Avg loss: 2.318249 \n",
      "\n",
      "batch 20, loss: 2.311055, accuracy: 9.0%\n",
      "Test Error: \n",
      "Accuracy: 7.6%, Avg loss: 2.316214 \n",
      "\n",
      "batch 21, loss: 2.303213, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 8.0%, Avg loss: 2.315436 \n",
      "\n",
      "batch 22, loss: 2.312756, accuracy: 9.0%\n",
      "Test Error: \n",
      "Accuracy: 8.1%, Avg loss: 2.315054 \n",
      "\n",
      "batch 23, loss: 2.321890, accuracy: 8.6%\n",
      "Test Error: \n",
      "Accuracy: 8.3%, Avg loss: 2.314564 \n",
      "\n",
      "batch 24, loss: 2.312856, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 8.4%, Avg loss: 2.314274 \n",
      "\n",
      "batch 25, loss: 2.299434, accuracy: 11.3%\n",
      "Test Error: \n",
      "Accuracy: 8.5%, Avg loss: 2.313725 \n",
      "\n",
      "batch 26, loss: 2.302288, accuracy: 6.6%\n",
      "Test Error: \n",
      "Accuracy: 8.6%, Avg loss: 2.313488 \n",
      "\n",
      "batch 27, loss: 2.308680, accuracy: 7.0%\n",
      "Test Error: \n",
      "Accuracy: 8.6%, Avg loss: 2.312016 \n",
      "\n",
      "batch 28, loss: 2.315730, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 8.7%, Avg loss: 2.310922 \n",
      "\n",
      "batch 29, loss: 2.320572, accuracy: 6.6%\n",
      "Test Error: \n",
      "Accuracy: 8.6%, Avg loss: 2.310144 \n",
      "\n",
      "batch 30, loss: 2.315866, accuracy: 6.6%\n",
      "Test Error: \n",
      "Accuracy: 8.8%, Avg loss: 2.309620 \n",
      "\n",
      "batch 31, loss: 2.324576, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 8.8%, Avg loss: 2.309253 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "batch 0, loss: 2.309896, accuracy: 10.2%\n",
      "Test Error: \n",
      "Accuracy: 8.8%, Avg loss: 2.309032 \n",
      "\n",
      "batch 1, loss: 2.306669, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 8.8%, Avg loss: 2.308629 \n",
      "\n",
      "batch 2, loss: 2.319515, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 8.8%, Avg loss: 2.308391 \n",
      "\n",
      "batch 3, loss: 2.305068, accuracy: 8.6%\n",
      "Test Error: \n",
      "Accuracy: 9.1%, Avg loss: 2.307759 \n",
      "\n",
      "batch 4, loss: 2.308120, accuracy: 9.0%\n",
      "Test Error: \n",
      "Accuracy: 8.9%, Avg loss: 2.307701 \n",
      "\n",
      "batch 5, loss: 2.305222, accuracy: 10.5%\n",
      "Test Error: \n",
      "Accuracy: 9.0%, Avg loss: 2.306458 \n",
      "\n",
      "batch 6, loss: 2.303142, accuracy: 12.1%\n",
      "Test Error: \n",
      "Accuracy: 9.0%, Avg loss: 2.306604 \n",
      "\n",
      "batch 7, loss: 2.312171, accuracy: 8.6%\n",
      "Test Error: \n",
      "Accuracy: 9.2%, Avg loss: 2.306105 \n",
      "\n",
      "batch 8, loss: 2.305677, accuracy: 10.5%\n",
      "Test Error: \n",
      "Accuracy: 9.2%, Avg loss: 2.305741 \n",
      "\n",
      "batch 9, loss: 2.305918, accuracy: 11.7%\n",
      "Test Error: \n",
      "Accuracy: 9.0%, Avg loss: 2.305906 \n",
      "\n",
      "batch 10, loss: 2.304558, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 9.1%, Avg loss: 2.305886 \n",
      "\n",
      "batch 11, loss: 2.303006, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 9.0%, Avg loss: 2.305145 \n",
      "\n",
      "batch 12, loss: 2.302596, accuracy: 8.2%\n",
      "Test Error: \n",
      "Accuracy: 9.0%, Avg loss: 2.304546 \n",
      "\n",
      "batch 13, loss: 2.311710, accuracy: 6.6%\n",
      "Test Error: \n",
      "Accuracy: 9.2%, Avg loss: 2.304384 \n",
      "\n",
      "batch 14, loss: 2.315859, accuracy: 8.2%\n",
      "Test Error: \n",
      "Accuracy: 9.2%, Avg loss: 2.304635 \n",
      "\n",
      "batch 15, loss: 2.300557, accuracy: 12.5%\n",
      "Test Error: \n",
      "Accuracy: 9.0%, Avg loss: 2.304879 \n",
      "\n",
      "batch 16, loss: 2.301085, accuracy: 10.2%\n",
      "Test Error: \n",
      "Accuracy: 9.1%, Avg loss: 2.304591 \n",
      "\n",
      "batch 17, loss: 2.303647, accuracy: 12.1%\n",
      "Test Error: \n",
      "Accuracy: 9.2%, Avg loss: 2.304268 \n",
      "\n",
      "batch 18, loss: 2.312231, accuracy: 8.6%\n",
      "Test Error: \n",
      "Accuracy: 9.2%, Avg loss: 2.304096 \n",
      "\n",
      "batch 19, loss: 2.300105, accuracy: 10.5%\n",
      "Test Error: \n",
      "Accuracy: 9.2%, Avg loss: 2.303422 \n",
      "\n",
      "batch 20, loss: 2.298892, accuracy: 7.0%\n",
      "Test Error: \n",
      "Accuracy: 9.2%, Avg loss: 2.302857 \n",
      "\n",
      "batch 21, loss: 2.300494, accuracy: 11.3%\n",
      "Test Error: \n",
      "Accuracy: 9.2%, Avg loss: 2.302822 \n",
      "\n",
      "batch 22, loss: 2.313280, accuracy: 9.4%\n",
      "Test Error: \n",
      "Accuracy: 9.4%, Avg loss: 2.302391 \n",
      "\n",
      "batch 23, loss: 2.306498, accuracy: 5.9%\n",
      "Test Error: \n",
      "Accuracy: 9.6%, Avg loss: 2.301998 \n",
      "\n",
      "batch 24, loss: 2.307069, accuracy: 10.9%\n",
      "Test Error: \n",
      "Accuracy: 9.7%, Avg loss: 2.301691 \n",
      "\n",
      "batch 25, loss: 2.308791, accuracy: 7.8%\n",
      "Test Error: \n",
      "Accuracy: 9.7%, Avg loss: 2.301539 \n",
      "\n",
      "batch 26, loss: 2.309892, accuracy: 10.2%\n",
      "Test Error: \n",
      "Accuracy: 9.7%, Avg loss: 2.301654 \n",
      "\n",
      "batch 27, loss: 2.305736, accuracy: 9.8%\n",
      "Test Error: \n",
      "Accuracy: 9.7%, Avg loss: 2.301316 \n",
      "\n",
      "batch 28, loss: 2.296555, accuracy: 10.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6270/3599845855.py\", line 31, in train_snn\n",
      "    train_loop_snn(\n",
      "  File \"/home/wyx/darwin_neuron/src/Training.py\", line 81, in train_loop_snn\n",
      "    val_stats = val_loop_snn(es_model, val_dataloader, loss_fn, device)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/darwin_neuron/src/Training.py\", line 52, in val_loop_snn\n",
      "    stats = run_snn_on_batch(model, x, y, loss_fn)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/darwin_neuron/src/Training.py\", line 25, in run_snn_on_batch\n",
      "    spikes, voltages = model(x)\n",
      "                       ^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/darwin_neuron/src/Models.py\", line 34, in forward\n",
      "    spk1, mem1 = self.lif1(self.fc1(x[t]), mem1)\n",
      "                           ^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Exception\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñá‚ñÖ‚ñà‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñá‚ñÖ</td></tr><tr><td>train_average_neuron_spikes</td><td>‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train_spike_percentage</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc</td><td>‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_average_neuron_spikes</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_spike_percentage</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.09766</td></tr><tr><td>train_average_neuron_spikes</td><td>0.02734</td></tr><tr><td>train_loss</td><td>2.30574</td></tr><tr><td>train_spike_percentage</td><td>0.02734</td></tr><tr><td>val_acc</td><td>0.0965</td></tr><tr><td>val_average_neuron_spikes</td><td>0.0289</td></tr><tr><td>val_loss</td><td>2.30132</td></tr><tr><td>val_spike_percentage</td><td>0.0288</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-4</strong> at: <a href='https://wandb.ai/yixing/ES-Randman10/runs/v2d3ock2' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10/runs/v2d3ock2</a><br> View project at: <a href='https://wandb.ai/yixing/ES-Randman10' target=\"_blank\">https://wandb.ai/yixing/ES-Randman10</a><br>Synced 5 W&B file(s), 0 media file(s), 120 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250617_202820-v2d3ock2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set sweep_id if picking up a sweep:\n",
    "# sweep_id = ryhbaq55\n",
    "wandb.agent(sweep_id, train_snn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "def train_snn():\n",
    "    run_name = \"sanity_check_1\"\n",
    "    config = {  # Dataset:\n",
    "        \"nb_input\": 100,\n",
    "        \"nb_output\": 10,\n",
    "        \"nb_steps\": 50,\n",
    "        \"nb_data_samples\": 1000,\n",
    "        # SNN:\n",
    "        \"nb_hidden\": 10,\n",
    "        \"learn_beta\": False,\n",
    "        # Evolution Strategy:\n",
    "        \"nb_model_samples\": 20,\n",
    "        \"mirror\": True,\n",
    "        # Training:\n",
    "        \"std\": 0.05,\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 256,\n",
    "        # Optimization:\n",
    "        \"loss\": \"cross-entropy\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"lr\": 0.01,\n",
    "        \"regularization\": \"none\",\n",
    "    }\n",
    "    with torch.no_grad(), wandb.init(\n",
    "        entity=\"DarwinNeuron\", project=\"ES-Randman10\", name=run_name, config=config\n",
    "    ) as run:\n",
    "        # initialize Evolution Strategy instance\n",
    "        es_model = ESModel(\n",
    "            RandmanSNN,\n",
    "            run.config.nb_input,\n",
    "            run.config.nb_hidden,\n",
    "            run.config.nb_output,\n",
    "            0.95,\n",
    "            sample_size=run.config.nb_model_samples,\n",
    "            param_std=run.config.std,\n",
    "            Optimizer=optim.Adam,\n",
    "            lr=run.config.lr,\n",
    "            device=device,\n",
    "            mirror=run.config.mirror,\n",
    "        )\n",
    "\n",
    "        # load dataset\n",
    "        train_loader, val_loader = read_randman10_dataset(\n",
    "            \"data/randman_10_dataset.pt\", batch_size=run.config.batch_size\n",
    "        )\n",
    "\n",
    "        # epochs\n",
    "        for epoch in range(run.config.epochs):\n",
    "            print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "\n",
    "            # train the model\n",
    "            train_loop_snn(\n",
    "                es_model, train_loader, val_loader, cross_entropy, device, run\n",
    "            )\n",
    "\n",
    "\n",
    "train_snn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattened Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight: torch.Size([10, 100])\n",
      "fc2.weight: torch.Size([10, 10])\n",
      "torch.Size([1100])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "\n",
    "def test_flatten():\n",
    "    model = RandmanSNN(100, 10, 10, False, 0.95)\n",
    "    for name, p in model.named_parameters():\n",
    "        print(f\"{name}: {p.shape}\")\n",
    "    vector = parameters_to_vector(model.parameters())\n",
    "    print(vector.shape)\n",
    "\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewESModel(EA):\n",
    "    \"\"\"\n",
    "    The class which keeps track of all the parameters in a model, handles sampling and updates\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self, Model, *model_args, sample_size=20, param_std=0.05, Optimizer = optim.Adam, lr = 0.01, mirror=True, device = default_device, **kwargs):    \n",
    "        \"\"\"initialize ESModel with the Model class and the standard deviation of the parameters. The means of the parameters are initialized to be the initial parameters of the model.\n",
    "\n",
    "        Args:\n",
    "            Model (nn.Module): the model whose parameters are to be optimized\n",
    "            param_std (float): standard deviation of the parameters, typically 0.01 to 0.05\n",
    "            Optimizer (torch.optim, optional): optimizer for ES. Defaults to optim.Adam.\n",
    "        \"\"\"\n",
    "        # self.param_dict has ('param_name', ESParam) pairs, each for one layer of the model\n",
    "        self.genome = parameters_to_vector(Model(*model_args).parameters())\n",
    "        self.sample_size = sample_size\n",
    "        self.Model = Model\n",
    "        self.model_args = model_args\n",
    "        self.device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try LEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leap_ec.simple import ea_solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"A real-valued function to be optimized.\"\"\"\n",
    "    return sum(x)**2\n",
    "\n",
    "ea_solve(f, bounds=[(-5.12, 5.12) for _ in range(5)], maximize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
