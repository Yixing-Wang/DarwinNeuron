{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb9e9c6-2c0f-4ed6-a625-fbfeecfa86e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myixing\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\w1886\\OneDrive\\Documents\\GitHub\\DarwinNeuron\\wandb\\run-20250813_163736-o30vxfhb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DarwinNeuron/Test/runs/o30vxfhb' target=\"_blank\">wenqi-pipeline-test</a></strong> to <a href='https://wandb.ai/DarwinNeuron/Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DarwinNeuron/Test' target=\"_blank\">https://wandb.ai/DarwinNeuron/Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DarwinNeuron/Test/runs/o30vxfhb' target=\"_blank\">https://wandb.ai/DarwinNeuron/Test/runs/o30vxfhb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath data/randman\\a818832ebb0748ed8125b520de22a658.pt\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "batch 0, loss: 2.302585, accuracy: 13.7%\n",
      "Accuracy: 10.2%, Avg loss: 2.302501 \n",
      "\n",
      "batch 1, loss: 2.301536, accuracy: 12.5%\n",
      "Accuracy: 10.2%, Avg loss: 2.302482 \n",
      "\n",
      "batch 2, loss: 2.302965, accuracy: 10.5%\n",
      "Accuracy: 10.2%, Avg loss: 2.302313 \n",
      "\n",
      "batch 3, loss: 2.302818, accuracy: 10.9%\n",
      "Accuracy: 10.3%, Avg loss: 2.302066 \n",
      "\n",
      "batch 4, loss: 2.301008, accuracy: 10.2%\n",
      "Accuracy: 10.4%, Avg loss: 2.301812 \n",
      "\n",
      "batch 5, loss: 2.301578, accuracy: 9.0%\n",
      "Accuracy: 10.5%, Avg loss: 2.301308 \n",
      "\n",
      "batch 6, loss: 2.302917, accuracy: 10.2%\n",
      "Accuracy: 10.7%, Avg loss: 2.300964 \n",
      "\n",
      "batch 7, loss: 2.301794, accuracy: 11.7%\n",
      "Accuracy: 10.9%, Avg loss: 2.300626 \n",
      "\n",
      "batch 8, loss: 2.298819, accuracy: 10.5%\n",
      "Accuracy: 11.2%, Avg loss: 2.299693 \n",
      "\n",
      "batch 9, loss: 2.298710, accuracy: 11.7%\n",
      "Accuracy: 11.5%, Avg loss: 2.298812 \n",
      "\n",
      "batch 10, loss: 2.294088, accuracy: 11.3%\n",
      "Accuracy: 11.8%, Avg loss: 2.297087 \n",
      "\n",
      "batch 11, loss: 2.295508, accuracy: 9.4%\n",
      "Accuracy: 12.0%, Avg loss: 2.296758 \n",
      "\n",
      "batch 12, loss: 2.289596, accuracy: 19.5%\n",
      "Accuracy: 12.4%, Avg loss: 2.295890 \n",
      "\n",
      "batch 13, loss: 2.291994, accuracy: 15.6%\n",
      "Accuracy: 12.5%, Avg loss: 2.295201 \n",
      "\n",
      "batch 14, loss: 2.283677, accuracy: 16.0%\n",
      "Accuracy: 12.8%, Avg loss: 2.294565 \n",
      "\n",
      "batch 15, loss: 2.295798, accuracy: 12.9%\n",
      "Accuracy: 13.0%, Avg loss: 2.294137 \n",
      "\n",
      "batch 16, loss: 2.290771, accuracy: 10.9%\n",
      "Accuracy: 12.9%, Avg loss: 2.294131 \n",
      "\n",
      "batch 17, loss: 2.285945, accuracy: 10.2%\n",
      "Accuracy: 13.2%, Avg loss: 2.293234 \n",
      "\n",
      "batch 18, loss: 2.279576, accuracy: 20.7%\n",
      "Accuracy: 13.5%, Avg loss: 2.291964 \n",
      "\n",
      "batch 19, loss: 2.287879, accuracy: 16.4%\n",
      "Accuracy: 13.5%, Avg loss: 2.291459 \n",
      "\n",
      "batch 20, loss: 2.293737, accuracy: 10.2%\n",
      "Accuracy: 13.7%, Avg loss: 2.290476 \n",
      "\n",
      "batch 21, loss: 2.286778, accuracy: 13.3%\n",
      "Accuracy: 14.0%, Avg loss: 2.289757 \n",
      "\n",
      "batch 22, loss: 2.292093, accuracy: 11.3%\n",
      "Accuracy: 13.9%, Avg loss: 2.289332 \n",
      "\n",
      "batch 23, loss: 2.281818, accuracy: 10.7%\n",
      "Accuracy: 13.9%, Avg loss: 2.288562 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "batch 0, loss: 2.284612, accuracy: 14.5%\n",
      "Accuracy: 14.2%, Avg loss: 2.287240 \n",
      "\n",
      "batch 1, loss: 2.288381, accuracy: 14.1%\n",
      "Accuracy: 14.1%, Avg loss: 2.286884 \n",
      "\n",
      "batch 2, loss: 2.282798, accuracy: 14.5%\n",
      "Accuracy: 14.1%, Avg loss: 2.286450 \n",
      "\n",
      "batch 3, loss: 2.280849, accuracy: 15.6%\n",
      "Accuracy: 14.1%, Avg loss: 2.286300 \n",
      "\n",
      "batch 4, loss: 2.276732, accuracy: 15.6%\n",
      "Accuracy: 14.0%, Avg loss: 2.285913 \n",
      "\n",
      "batch 5, loss: 2.288526, accuracy: 14.1%\n",
      "Accuracy: 13.9%, Avg loss: 2.285860 \n",
      "\n",
      "batch 6, loss: 2.281071, accuracy: 13.3%\n",
      "Accuracy: 14.0%, Avg loss: 2.285505 \n",
      "\n",
      "batch 7, loss: 2.275975, accuracy: 16.4%\n",
      "Accuracy: 14.2%, Avg loss: 2.284415 \n",
      "\n",
      "batch 8, loss: 2.288745, accuracy: 16.0%\n",
      "Accuracy: 13.9%, Avg loss: 2.284505 \n",
      "\n",
      "batch 9, loss: 2.284672, accuracy: 15.2%\n",
      "Accuracy: 13.8%, Avg loss: 2.283951 \n",
      "\n",
      "batch 10, loss: 2.278407, accuracy: 17.2%\n",
      "Accuracy: 13.7%, Avg loss: 2.284032 \n",
      "\n",
      "batch 11, loss: 2.274053, accuracy: 17.2%\n",
      "Accuracy: 13.4%, Avg loss: 2.284435 \n",
      "\n",
      "batch 12, loss: 2.272217, accuracy: 19.5%\n",
      "Accuracy: 13.6%, Avg loss: 2.284552 \n",
      "\n",
      "batch 13, loss: 2.283440, accuracy: 15.2%\n",
      "Accuracy: 13.7%, Avg loss: 2.284423 \n",
      "\n",
      "batch 14, loss: 2.276360, accuracy: 15.2%\n",
      "Accuracy: 14.0%, Avg loss: 2.283202 \n",
      "\n",
      "batch 15, loss: 2.270844, accuracy: 18.8%\n",
      "Accuracy: 14.1%, Avg loss: 2.280691 \n",
      "\n",
      "batch 16, loss: 2.276273, accuracy: 18.0%\n",
      "Accuracy: 14.3%, Avg loss: 2.279933 \n",
      "\n",
      "batch 17, loss: 2.273679, accuracy: 14.1%\n",
      "Accuracy: 14.6%, Avg loss: 2.278494 \n",
      "\n",
      "batch 18, loss: 2.252056, accuracy: 20.7%\n",
      "Accuracy: 14.9%, Avg loss: 2.276731 \n",
      "\n",
      "batch 19, loss: 2.280233, accuracy: 15.6%\n",
      "Accuracy: 15.1%, Avg loss: 2.275740 \n",
      "\n",
      "batch 20, loss: 2.273070, accuracy: 14.5%\n",
      "Accuracy: 15.4%, Avg loss: 2.274049 \n",
      "\n",
      "batch 21, loss: 2.260940, accuracy: 18.0%\n",
      "Accuracy: 15.7%, Avg loss: 2.272625 \n",
      "\n",
      "batch 22, loss: 2.262815, accuracy: 17.6%\n",
      "Accuracy: 15.8%, Avg loss: 2.271682 \n",
      "\n",
      "batch 23, loss: 2.233178, accuracy: 18.8%\n",
      "Accuracy: 16.4%, Avg loss: 2.269392 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "batch 0, loss: 2.252197, accuracy: 18.0%\n",
      "Accuracy: 16.5%, Avg loss: 2.268559 \n",
      "\n",
      "batch 1, loss: 2.262884, accuracy: 16.4%\n",
      "Accuracy: 16.7%, Avg loss: 2.265908 \n",
      "\n",
      "batch 2, loss: 2.256533, accuracy: 18.0%\n",
      "Accuracy: 16.7%, Avg loss: 2.264972 \n",
      "\n",
      "batch 3, loss: 2.264667, accuracy: 16.8%\n",
      "Accuracy: 16.6%, Avg loss: 2.264023 \n",
      "\n",
      "batch 4, loss: 2.245171, accuracy: 18.8%\n",
      "Accuracy: 16.2%, Avg loss: 2.263928 \n",
      "\n",
      "batch 5, loss: 2.256482, accuracy: 18.0%\n",
      "Accuracy: 16.9%, Avg loss: 2.262680 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\w1886\\AppData\\Local\\Temp\\ipykernel_25996\\1159165513.py\", line 92, in train_snn\n",
      "    train_loop_snn(my_model, train_loader, val_loader, loss_fn_spk, device, run, epoch, loss_plotter=None)\n",
      "  File \"c:\\Users\\w1886\\OneDrive\\Documents\\GitHub\\DarwinNeuron\\src\\Training.py\", line 43, in train_loop_snn\n",
      "    my_model.update(get_model_stats)\n",
      "  File \"c:\\Users\\w1886\\OneDrive\\Documents\\GitHub\\DarwinNeuron\\src\\EvolutionAlgorithms\\EvolutionStrategy.py\", line 135, in update\n",
      "    samples_loss.append(model_stats_fn(model).loss)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\OneDrive\\Documents\\GitHub\\DarwinNeuron\\src\\Training.py\", line 41, in get_model_stats\n",
      "    stats = run_snn_on_batch(model, x, y, loss_fn)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\OneDrive\\Documents\\GitHub\\DarwinNeuron\\src\\Utilities.py\", line 138, in run_snn_on_batch\n",
      "    spikes, voltages = model(x)\n",
      "                       ^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\OneDrive\\Documents\\GitHub\\DarwinNeuron\\src\\Models.py\", line 50, in forward\n",
      "    _, mem2 = self.lif2(self.fc2(spk1), mem2)\n",
      "                        ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅████</td></tr><tr><td>epoch_val_acc</td><td>▁█</td></tr><tr><td>epoch_val_average_neuron_spikes</td><td>▁█</td></tr><tr><td>epoch_val_loss</td><td>█▁</td></tr><tr><td>epoch_val_spike_percentage</td><td>▁█</td></tr><tr><td>train_acc</td><td>▃▃▁▂▁▁▂▁▂▂▇▅▅▃▂▁▂▁▄▄▅▅▄▃▅▆▆▇▄▄▄█▄▆▆▆▅▆▅▆</td></tr><tr><td>train_average_neuron_spikes</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▄▃▄▄▅▅▆▆▇▆█▇▇██</td></tr><tr><td>train_loss</td><td>███████▇▇▇▆▅▇▆▅▇▆▇▅▆▅▄▆▅▄▄▄▅▄▄▁▅▄▂▂▁▂▂▃▂</td></tr><tr><td>train_spike_percentage</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▅▅▆▆▆▆▇▇██▇</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▂▂▂▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇█████</td></tr><tr><td>val_average_neuron_spikes</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▆▆▆▇▇▇████</td></tr><tr><td>val_loss</td><td>██████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▄▄▄▃▃▃▃▂▂▁▁▁</td></tr><tr><td>val_spike_percentage</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▆▆▆▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>epoch_val_acc</td><td>0.164</td></tr><tr><td>epoch_val_average_neuron_spikes</td><td>0.01091</td></tr><tr><td>epoch_val_loss</td><td>2.26939</td></tr><tr><td>epoch_val_spike_percentage</td><td>0.01091</td></tr><tr><td>train_acc</td><td>0.17969</td></tr><tr><td>train_average_neuron_spikes</td><td>0.01219</td></tr><tr><td>train_loss</td><td>2.25648</td></tr><tr><td>train_spike_percentage</td><td>0.01219</td></tr><tr><td>val_acc</td><td>0.169</td></tr><tr><td>val_average_neuron_spikes</td><td>0.01234</td></tr><tr><td>val_loss</td><td>2.26268</td></tr><tr><td>val_spike_percentage</td><td>0.01234</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">256-loss_fn-0.01-es-10-100-0.1</strong> at: <a href='https://wandb.ai/DarwinNeuron/Test/runs/o30vxfhb' target=\"_blank\">https://wandb.ai/DarwinNeuron/Test/runs/o30vxfhb</a><br> View project at: <a href='https://wandb.ai/DarwinNeuron/Test' target=\"_blank\">https://wandb.ai/DarwinNeuron/Test</a><br>Synced 5 W&B file(s), 0 media file(s), 112 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250813_163736-o30vxfhb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     91\u001b[39m             \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[32m     92\u001b[39m             train_loop_snn(my_model, train_loader, val_loader, loss_fn_spk, device, run, epoch, loss_plotter=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m train_snn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mtrain_snn\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m train_loop_snn(my_model, train_loader, val_loader, loss_fn_spk, device, run, epoch, loss_plotter=\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\OneDrive\\Documents\\GitHub\\DarwinNeuron\\src\\Training.py:43\u001b[39m, in \u001b[36mtrain_loop_snn\u001b[39m\u001b[34m(my_model, train_dataloader, val_dataloader, loss_fn, device, run, epoch, results_path, loss_plotter)\u001b[39m\n\u001b[32m     41\u001b[39m         stats = run_snn_on_batch(model, x, y, loss_fn)\n\u001b[32m     42\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m stats\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     my_model.update(get_model_stats)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m## best model metrics as training metrics\u001b[39;00m\n\u001b[32m     46\u001b[39m best_model = my_model.get_best_model()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\OneDrive\\Documents\\GitHub\\DarwinNeuron\\src\\EvolutionAlgorithms\\EvolutionStrategy.py:135\u001b[39m, in \u001b[36mESModel.update\u001b[39m\u001b[34m(self, model_stats_fn)\u001b[39m\n\u001b[32m    133\u001b[39m samples_loss = []\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.samples():\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     samples_loss.append(model_stats_fn(model).loss)            \n\u001b[32m    137\u001b[39m samples_loss = torch.stack(samples_loss) \n\u001b[32m    138\u001b[39m \u001b[38;5;28mself\u001b[39m.gradient_descent(samples_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\OneDrive\\Documents\\GitHub\\DarwinNeuron\\src\\Training.py:41\u001b[39m, in \u001b[36mtrain_loop_snn.<locals>.get_model_stats\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_model_stats\u001b[39m(model):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     stats = run_snn_on_batch(model, x, y, loss_fn)\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stats\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\OneDrive\\Documents\\GitHub\\DarwinNeuron\\src\\Utilities.py:138\u001b[39m, in \u001b[36mrun_snn_on_batch\u001b[39m\u001b[34m(model, x, y, loss_fn)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_snn_on_batch\u001b[39m(model, x, y, loss_fn): \n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# shape: [time_steps, batch_size, classes]\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(): \u001b[38;5;66;03m# EA forward doesn't need gradient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         spikes, voltages = model(x)\n\u001b[32m    139\u001b[39m         pred_y = spike_to_label(voltages, scheme = \u001b[33m'\u001b[39m\u001b[33mhighest_voltage\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    140\u001b[39m         logits = voltage_to_logits(voltages, scheme=\u001b[33m'\u001b[39m\u001b[33mhighest-voltage\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\OneDrive\\Documents\\GitHub\\DarwinNeuron\\src\\Models.py:50\u001b[39m, in \u001b[36mRandmanSNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     49\u001b[39m     spk1, mem1 = \u001b[38;5;28mself\u001b[39m.lif1(\u001b[38;5;28mself\u001b[39m.fc1(x[t]), mem1)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m _, mem2 = \u001b[38;5;28mself\u001b[39m.lif2(\u001b[38;5;28mself\u001b[39m.fc2(spk1), mem2)\n\u001b[32m     52\u001b[39m spk1_rec.append(spk1)\n\u001b[32m     53\u001b[39m mem2_rec.append(mem2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch, wandb, os\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.RandmanFunctions import RandmanConfig, split_and_load\n",
    "from src.Models import RandmanSNN, spike_regularized_cross_entropy\n",
    "from src.EvolutionAlgorithms.EvolutionStrategy import ESModel\n",
    "from src.EvolutionAlgorithms.PseudoPSO import PPSOModel, PPSOModelWithPooling\n",
    "from src.SurrogateGD.VanillaSGD import SGDModel\n",
    "from src.Training import train_loop_snn\n",
    "from src.Utilities import init_result_csv, set_seed\n",
    "from src.LandscapeAnalysis.LossSurfacePlotter import LossSurfacePlotter\n",
    "\n",
    "device = 'cuda'\n",
    "loss_fn_spk = lambda logits, y, spikes: spike_regularized_cross_entropy(logits, y, spikes, lambda_reg=1e-3)\n",
    "\n",
    "@torch.no_grad()\n",
    "def train_snn():\n",
    "    run_name = \"wenqi-pipeline-test\"\n",
    "    config = {  # Dataset:\n",
    "        \"nb_input\": 10,\n",
    "        \"nb_output\": 10,\n",
    "        \"nb_steps\": 50,\n",
    "        \"nb_data_samples\": 1000,\n",
    "        \"dim_manifold\": 2,\n",
    "        \"alpha\": 2.0,\n",
    "        # SNN:\n",
    "        \"nb_hidden\": 100,\n",
    "        \"learn_beta\": False,\n",
    "        # Evolution Strategy:\n",
    "        \"nb_model_samples\": 100,\n",
    "        \"mirror\": True,\n",
    "        # Training:\n",
    "        \"std\": 0.1,\n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": 256,\n",
    "        \"method\": \"es\",\n",
    "        # Optimization:\n",
    "        \"loss\": \"loss_fn\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"lr\": 0.01,\n",
    "        \"regularization\": \"none\",\n",
    "    }\n",
    "    with wandb.init(\n",
    "        entity=\"DarwinNeuron\", project=\"Test\", name=run_name, config=config\n",
    "    ) as run:\n",
    "        # update current run_name\n",
    "        keys = [\"method\", \"std\", \"batch_size\", \"lr\", \"nb_model_samples\", \"loss\", \"nb_input\"]\n",
    "        sorted_items = [f\"{getattr(run.config, k)}\" for k in sorted(keys)]\n",
    "        run.name = \"-\".join(sorted_items)\n",
    "        \n",
    "        # setting up local csv recording (optional)\n",
    "        # result_path, _, _ = init_result_csv(dict(run.config), run.project)\n",
    "\n",
    "        my_model = ESModel(\n",
    "            RandmanSNN,\n",
    "            run.config.nb_input,\n",
    "            run.config.nb_hidden,\n",
    "            run.config.nb_output,\n",
    "            run.config.learn_beta, \n",
    "            sample_size=run.config.nb_model_samples,\n",
    "            param_std=run.config.std,\n",
    "            Optimizer=optim.Adam,\n",
    "            lr=run.config.lr,\n",
    "            device=device,\n",
    "            mirror=run.config.mirror,\n",
    "        )\n",
    "\n",
    "        # load dataset\n",
    "        train_loader, val_loader = split_and_load(\n",
    "            RandmanConfig(\n",
    "                nb_classes=run.config.nb_output,\n",
    "                nb_units=run.config.nb_input,\n",
    "                nb_steps=run.config.nb_steps,\n",
    "                nb_samples=run.config.nb_data_samples,\n",
    "                dim_manifold=run.config.dim_manifold,\n",
    "                alpha=run.config.alpha,\n",
    "            ).read_dataset(),\n",
    "            run.config.batch_size,\n",
    "        )\n",
    "        \n",
    "        # loss surface plotter\n",
    "        plotter_dir = f\"results/{run.project}/runs/{run.id}/\"\n",
    "        os.makedirs(plotter_dir, exist_ok=True)\n",
    "        loss_plotter = LossSurfacePlotter(plotter_dir+\"illuminated_loss_surface.npz\")\n",
    "\n",
    "        # epochs\n",
    "        for epoch in range(run.config.epochs):\n",
    "            print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "\n",
    "            # train the model\n",
    "            train_loop_snn(my_model, train_loader, val_loader, loss_fn_spk, device, run, epoch, loss_plotter=None)\n",
    "\n",
    "train_snn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201e6fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myixing\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m run = wandb.init(\n\u001b[32m      3\u001b[39m     entity=\u001b[33m\"\u001b[39m\u001b[33mDarwinNeuron\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     project=\u001b[33m\"\u001b[39m\u001b[33mTest\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mwandb-connection-diag\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     config={\u001b[33m\"\u001b[39m\u001b[33mdiag\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[32m      7\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\wandb\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1581\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[39m\n\u001b[32m   1578\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_settings.x_server_side_derived_summary:\n\u001b[32m   1579\u001b[39m             init_telemetry.feature.server_side_derived_summary = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1581\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m wi.init(run_settings, run_config, run_printer)\n\u001b[32m   1583\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1584\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wl:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\wandb\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:996\u001b[39m, in \u001b[36m_WandbInit.init\u001b[39m\u001b[34m(self, settings, config, run_printer)\u001b[39m\n\u001b[32m    990\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m progress.loop_printing_operation_stats(\n\u001b[32m    991\u001b[39m             progress_printer,\n\u001b[32m    992\u001b[39m             backend.interface,\n\u001b[32m    993\u001b[39m         )\n\u001b[32m    995\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     result = wait_with_progress(\n\u001b[32m    997\u001b[39m         run_init_handle,\n\u001b[32m    998\u001b[39m         timeout=timeout,\n\u001b[32m    999\u001b[39m         progress_after=\u001b[32m1\u001b[39m,\n\u001b[32m   1000\u001b[39m         display_progress=display_init_message,\n\u001b[32m   1001\u001b[39m     )\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[32m   1004\u001b[39m     run_init_handle.cancel(backend.interface)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\wandb\\Lib\\site-packages\\wandb\\sdk\\mailbox\\wait_with_progress.py:24\u001b[39m, in \u001b[36mwait_with_progress\u001b[39m\u001b[34m(handle, timeout, progress_after, display_progress)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait_with_progress\u001b[39m(\n\u001b[32m     14\u001b[39m     handle: MailboxHandle[_T],\n\u001b[32m     15\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     display_progress: Callable[[], Coroutine[Any, Any, \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[32m     19\u001b[39m ) -> _T:\n\u001b[32m     20\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wait for a handle, possibly displaying progress to the user.\u001b[39;00m\n\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m \u001b[33;03m    Equivalent to passing a single handle to `wait_all_with_progress`.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m wait_all_with_progress(\n\u001b[32m     25\u001b[39m         [handle],\n\u001b[32m     26\u001b[39m         timeout=timeout,\n\u001b[32m     27\u001b[39m         progress_after=progress_after,\n\u001b[32m     28\u001b[39m         display_progress=display_progress,\n\u001b[32m     29\u001b[39m     )[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\wandb\\Lib\\site-packages\\wandb\\sdk\\mailbox\\wait_with_progress.py:87\u001b[39m, in \u001b[36mwait_all_with_progress\u001b[39m\u001b[34m(handle_list, timeout, progress_after, display_progress)\u001b[39m\n\u001b[32m     80\u001b[39m             remaining_timeout = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     82\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait_handles_async(\n\u001b[32m     83\u001b[39m             handle_list,\n\u001b[32m     84\u001b[39m             timeout=remaining_timeout,\n\u001b[32m     85\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m asyncio_compat.run(progress_loop_with_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\wandb\\Lib\\site-packages\\wandb\\sdk\\lib\\asyncio_compat.py:30\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m     27\u001b[39m future = executor.submit(runner.run, fn)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m future.result()\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     33\u001b[39m     runner.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\wandb\\Lib\\concurrent\\futures\\_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\wandb\\Lib\\threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         waiter.acquire()\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init(\n",
    "    entity=\"DarwinNeuron\",\n",
    "    project=\"Test\",\n",
    "    name=\"wandb-connection-diag\",\n",
    "    config={\"diag\": True},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
