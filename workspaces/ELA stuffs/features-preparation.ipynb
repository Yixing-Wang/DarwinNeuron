{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea28afc",
   "metadata": {},
   "source": [
    "# Experiment 1: Varying Randman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9e3a1",
   "metadata": {},
   "source": [
    "## Generate randman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db46e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.RandmanFunctions import RandmanConfig, generate_and_save_randman\n",
    "\n",
    "data_configs = [{'nb_classes': 2, 'nb_units': 3, 'alpha': 3, 'dim_manifold': 1},\n",
    "                {'nb_classes': 5, 'nb_units': 3, 'alpha': 3, 'dim_manifold': 1},\n",
    "                {'nb_classes': 2, 'nb_units': 10, 'alpha': 3, 'dim_manifold': 1},\n",
    "                {'nb_classes': 2, 'nb_units': 3, 'alpha': 1, 'dim_manifold': 1},\n",
    "                {'nb_classes': 2, 'nb_units': 3, 'alpha': 3, 'dim_manifold': 2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_config in data_configs:\n",
    "    generate_and_save_randman(RandmanConfig(**data_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4032b1b9",
   "metadata": {},
   "source": [
    "## Generate problem\n",
    "\n",
    "### BBOB problem\n",
    "What BBOB problems should be generated? \n",
    "* 24 functions\n",
    "* dims: 2, 160, 640 \n",
    "* 3 instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00441883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.LandscapeAnalysis import BBOBProblemConfig\n",
    "def generate_bbob():\n",
    "    # Generate all combinations of BBOB problems with the given configurations\n",
    "    functions = list(range(1, 25))  # 24 functions\n",
    "    dimensions = [2, 160, 640]      # 3 dimensions\n",
    "    instances = [1, 2, 3]           # 3 instances\n",
    "\n",
    "    for function in functions:\n",
    "        for dim in dimensions:\n",
    "            for instance in instances:\n",
    "                problem_config = BBOBProblemConfig(function, instance, dim)\n",
    "                problem_config.write_to_db(db_path='data/landscape-analysis.db')\n",
    "generate_bbob()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67be258",
   "metadata": {},
   "source": [
    "### Generate samples and match with problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a8c5d",
   "metadata": {},
   "source": [
    "How many samples should be used? Why not 30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddcff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from math import ceil, log2\n",
    "from src.LandscapeAnalysis import ParameterSampleConfig, assign_samples_to_problem, BBOBProblemConfig\n",
    "\n",
    "def assign_samples_to_bbob():\n",
    "    # Connect to the SQLite database\n",
    "    db_path = \"data/landscape-analysis.db\"\n",
    "    con = sqlite3.connect(db_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute(\"SELECT id, dim FROM bbob_problems\")\n",
    "    rows = cur.fetchall()\n",
    "    con.close()\n",
    "\n",
    "    for problem_id, dim in rows:\n",
    "        nb_samples = 2**(ceil(log2(50 * dim)))  # sobol's sample size must be a power of 2\n",
    "        max_nb_versions = 30\n",
    "        lower_bound = -5 # bbob are defined on [-5, 5]\n",
    "        upper_bound = 5\n",
    "        sample_config = ParameterSampleConfig(dim, nb_samples, \"sobol\", lower_bound, upper_bound)\n",
    "        problem_config = BBOBProblemConfig.lookup_by_id(problem_id)\n",
    "        assign_samples_to_problem(problem_config, sample_config, max_nb_versions, 'data/samples', 'data/landscape-analysis.db')\n",
    "assign_samples_to_bbob()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c6ccf",
   "metadata": {},
   "source": [
    "## Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.LandscapeAnalysis import get_next_available_id, calculate_and_save_loss\n",
    "\n",
    "while (id := get_next_available_id('loss_filename')) is not None:\n",
    "    calculate_and_save_loss(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579f4d7",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282bbd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'ic_h_max' added to 'loss_surfaces' table by get_next_available_id()\n"
     ]
    }
   ],
   "source": [
    "from pflacco.classical_ela_features import *\n",
    "from src.LandscapeAnalysis import get_next_available_id, calculate_and_save_features\n",
    "while (id := get_next_available_id('ic_h_max')) is not None:\n",
    "    calculate_and_save_features(id, calculate_information_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2561070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r20_ela_meta.lin_simple.adj_r2': -0.005409156310980379, 'r20_ela_meta.lin_simple.intercept': 0.4715494876893619, 'r20_ela_meta.lin_simple.coef.min': 0.0020344682591832736, 'r20_ela_meta.lin_simple.coef.max': 0.08213140207973536, 'r20_ela_meta.lin_simple.coef.max_by_min': 40.36995991901421, 'r20_ela_meta.lin_w_interact.adj_r2': -0.019009364039383136, 'r20_ela_meta.quad_simple.adj_r2': -0.0057795330142678125, 'r20_ela_meta.quad_simple.cond': 5.64899122378976, 'r20_ela_meta.quad_w_interact.adj_r2': 0.002901501956317998, 'r20_ela_meta.costs_runtime': 0.015}\n"
     ]
    }
   ],
   "source": [
    "from pflacco.classical_ela_features import *\n",
    "import numpy as np\n",
    "\n",
    "def find_feature_name():\n",
    "    from pflacco.sampling import create_initial_sample\n",
    "\n",
    "    # Arbitrary objective function\n",
    "    def objective_function(x):\n",
    "        return np.random.rand()\n",
    "\n",
    "    dim = 3\n",
    "    # Create inital sample using latin hyper cube sampling\n",
    "    X = create_initial_sample(dim, sample_type = 'lhs')\n",
    "    # Calculate the objective values of the initial sample\n",
    "    # using an arbitrary objective function\n",
    "    y = X.apply(lambda x: objective_function(x), axis = 1)\n",
    "\n",
    "    # Compute the 3 feature sets from the classical ELA features which are solely based on the initial sample\n",
    "    ela_meta = calculate_ela_meta(X, y)\n",
    "    print(ela_meta)\n",
    "find_feature_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe7531e",
   "metadata": {},
   "source": [
    "# Experiment 2: Varying SNN\n",
    "\n",
    "## Generate nn problems\n",
    "\n",
    "The dataset would be the easiest where `nb_input`= 10, `nb_classes`=2, `alpha`=3. The `randman_id`=16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcfaa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.RandmanFunctions import RandmanConfig\n",
    "from src.Models import RandmanSNNConfig\n",
    "from src.LandscapeAnalysis import generate_randman_problem\n",
    "\n",
    "def add_nn_problem_configs():\n",
    "    # The easiest randman\n",
    "    randman_config = RandmanConfig.lookup_by_id(16) \n",
    "\n",
    "    ## Now the SNN configurations\n",
    "    # constants in this experiment\n",
    "    BETA = 0.95\n",
    "    LEARN_BETA = True\n",
    "    PARAMETER_TYPE = 'weights'\n",
    "    \n",
    "    # variables in this experiment\n",
    "    nb_hidden_1_list = [100, 100, 30, 100]\n",
    "    recurrent_list = [False, True, False, False]\n",
    "    nb_hidden_2_list = [-1, -1, -1, 10]\n",
    "    for i in range(len(nb_hidden_1_list)):\n",
    "        snn_config = RandmanSNNConfig(\n",
    "            nb_hidden_1=nb_hidden_1_list[i],\n",
    "            nb_hidden_2=nb_hidden_2_list[i],\n",
    "            beta=BETA,\n",
    "            learn_beta=LEARN_BETA,\n",
    "            recurrent=recurrent_list[i],\n",
    "            parameter_type=PARAMETER_TYPE\n",
    "        )\n",
    "        generate_randman_problem(randman_config, snn_config, loss_fn= 'cross_entropy', db_path='data/landscape-analysis.db')\n",
    "\n",
    "add_nn_problem_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f4e07",
   "metadata": {},
   "source": [
    "## Generate samples and assign to problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c6c92",
   "metadata": {},
   "source": [
    "Let's me try to stick with 8192 samples for this experiment becuase the highest dim is 11202."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.LandscapeAnalysis import ParameterSampleConfig, NNProblemConfig, assign_samples_to_problem\n",
    "def generate_and_assign():\n",
    "    NB_SAMPLES = 8192\n",
    "    LOWER_BOUND = -2\n",
    "    UPPER_BOUND = 2\n",
    "    \n",
    "    NB_VERSIONS = 30\n",
    "    for problem_id in (5, 6, 7, 8):\n",
    "        problem_config = NNProblemConfig.lookup_by_id(problem_id)\n",
    "        sample_config = ParameterSampleConfig(\n",
    "            dim = problem_config.dim,\n",
    "            nb_sample = NB_SAMPLES,\n",
    "            method = 'sobol',\n",
    "            lower_bound = LOWER_BOUND,\n",
    "            upper_bound = UPPER_BOUND\n",
    "        )\n",
    "        assign_samples_to_problem(problem_config, sample_config, NB_VERSIONS, 'data/new_samples', 'data/landscape-analysis.db')\n",
    "generate_and_assign()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2bd1e2",
   "metadata": {},
   "source": [
    "### A problem to think about\n",
    "\n",
    "Let's assume that the ELA features can relieably distinguish different problems, good. But what if the reason is, as we saw from the bbob problems, solely due to the dimensionality. What is the consequences of that, then? First of all, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a4e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath data/randman\\aa13286ca3664a0cbb1ae916d917ba93.pt\n",
      "calculating loss for loss_surface_id 6632 with 8192 samples\n",
      "Accuracy: 49.5%, Avg loss: 12.145149 \n",
      "\n",
      "Accuracy: 50.5%, Avg loss: 3.699378 \n",
      "\n",
      "Accuracy: 48.8%, Avg loss: 2.253944 \n",
      "\n",
      "Accuracy: 49.3%, Avg loss: 1.372035 \n",
      "\n",
      "Accuracy: 49.5%, Avg loss: 4.821850 \n",
      "\n",
      "Accuracy: 50.5%, Avg loss: 6.054319 \n",
      "\n",
      "Accuracy: 45.1%, Avg loss: 2.965990 \n",
      "\n",
      "Accuracy: 40.8%, Avg loss: 4.377857 \n",
      "\n",
      "Accuracy: 49.5%, Avg loss: 3.386114 \n",
      "\n",
      "Accuracy: 51.6%, Avg loss: 2.526793 \n",
      "\n",
      "Accuracy: 50.1%, Avg loss: 2.664882 \n",
      "\n",
      "Accuracy: 58.9%, Avg loss: 5.772611 \n",
      "\n",
      "Accuracy: 50.2%, Avg loss: 11.803179 \n",
      "\n",
      "Accuracy: 53.0%, Avg loss: 2.352435 \n",
      "\n",
      "Accuracy: 55.7%, Avg loss: 2.269084 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLandscapeAnalysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calculate_and_save_loss\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m calculate_and_save_loss(\u001b[32m6632\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\OneDrive\\My Documents\\Projects\\darwin_neuron\\src\\LandscapeAnalysis\\Pipeline.py:531\u001b[39m, in \u001b[36mcalculate_and_save_loss\u001b[39m\u001b[34m(loss_surface_id, sample_dir, randman_dir, loss_dir, db_path, device)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# The computation part\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcalculating loss for loss_surface_id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_surface_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m loss = np.apply_along_axis(f, \u001b[32m1\u001b[39m, samples)\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Save loss to the database\u001b[39;00m\n\u001b[32m    534\u001b[39m loss_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid.uuid4().hex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<__array_function__ internals>:200\u001b[39m, in \u001b[36mapply_along_axis\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\anaconda3\\envs\\darwin\\Lib\\site-packages\\numpy\\lib\\shape_base.py:402\u001b[39m, in \u001b[36mapply_along_axis\u001b[39m\u001b[34m(func1d, axis, arr, *args, **kwargs)\u001b[39m\n\u001b[32m    400\u001b[39m buff[ind0] = res\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m inds:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     buff[ind] = asanyarray(func1d(inarr_view[ind], *args, **kwargs))\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, matrix):\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# wrap the array, to preserve subclasses\u001b[39;00m\n\u001b[32m    406\u001b[39m     buff = res.__array_wrap__(buff)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\OneDrive\\My Documents\\Projects\\darwin_neuron\\src\\LandscapeAnalysis\\Utils.py:26\u001b[39m, in \u001b[36mget_parameter_to_loss_fn.<locals>.parameter_to_loss_fn\u001b[39m\u001b[34m(vector)\u001b[39m\n\u001b[32m     24\u001b[39m vector = torch.tensor(vector, dtype=torch.float32, device=device)\n\u001b[32m     25\u001b[39m vector_to_parameters(vector, model.parameters())\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m stats = evaluate_snn(model, loader, loss, device)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stats.loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\OneDrive\\My Documents\\Projects\\darwin_neuron\\src\\Utilities.py:180\u001b[39m, in \u001b[36mevaluate_snn\u001b[39m\u001b[34m(model, dataloader, loss_fn, device)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m    179\u001b[39m     x, y = x.to(device), y.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     stats = run_snn_on_batch(model, x, y, loss_fn) \n\u001b[32m    181\u001b[39m     test_loss += stats.loss\n\u001b[32m    182\u001b[39m     correct += stats.correct\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\w1886\\OneDrive\\My Documents\\Projects\\darwin_neuron\\src\\Utilities.py:153\u001b[39m, in \u001b[36mrun_snn_on_batch\u001b[39m\u001b[34m(model, x, y, loss_fn)\u001b[39m\n\u001b[32m    150\u001b[39m stats = SNNStats(loss, correct, \u001b[38;5;28mlen\u001b[39m(y), spike_count_per_neuron)\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m spikes, voltages, pred_y, logits, loss\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m gc.collect()\n\u001b[32m    154\u001b[39m torch.cuda.empty_cache()\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stats\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from src.LandscapeAnalysis import calculate_and_save_loss\n",
    "calculate_and_save_loss(6632)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
