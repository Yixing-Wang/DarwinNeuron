import sys, os, torch, uuid
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
import randman
import numpy as np
import pandas as pd
from dataclasses import dataclass, asdict

from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split

from src.Utilities import next_id, match_config

SEED = 42

def standardize(x,eps=1e-7):
    # x's (which is actually y in the following code) shape will be [samples, units]
    # Therefore, 0-axis shows that the author standardize across all samples for each units
    mi,_ = x.min(0)
    ma,_ = x.max(0)
    return (x-mi)/(ma-mi+eps)

def make_spiking_dataset(nb_classes=10, nb_units=100, nb_steps=100, step_frac=1.0, dim_manifold=2, nb_spikes=1, nb_samples=1000, alpha=2.0, shuffle=True, classification=True, seed=None):
    """ Generates event-based generalized spiking randman classification/regression dataset. 
    In this dataset each unit fires a fixed number of spikes. So ratebased or spike count based decoding won't work. 
    All the information is stored in the relative timing between spikes.
    For regression datasets the intrinsic manifold coordinates are returned for each target.
    Args: 
        nb_classes: The number of classes to generate
        nb_units: The number of units to assume
        nb_steps: The number of time steps to assume
        step_frac: Fraction of time steps from beginning of each to contain spikes (default 1.0)
        nb_spikes: The number of spikes per unit
        nb_samples: Number of samples from each manifold per class
        alpha: Randman smoothness parameter
        shuffe: Whether to shuffle the dataset
        classification: Whether to generate a classification (default) or regression dataset
        seed: The random seed (default: None)
    Returns: 
        A tuple of data,labels. The data is structured as numpy array 
        (sample x event x 2 ) where the last dimension contains    
        the relative [0,1] (time,unit) coordinates and labels.
    """
  
    data = []
    labels = []
    targets = []
    
    if SEED is not None:
        np.random.seed(SEED)
    
    max_value = np.iinfo(np.int32).max
    randman_seeds = np.random.randint(max_value, size=(nb_classes,nb_spikes) )

    for k in range(nb_classes):
        x = np.random.rand(nb_samples,dim_manifold)
        
        # The following code shows that if more than one spike, different spikes, even for the same unit, are generated by independent mappings 
        submans = [ randman.Randman(nb_units, dim_manifold, alpha=alpha, seed=randman_seeds[k,i]) for i in range(nb_spikes) ]
        units = []
        times = []
        for i,rm in enumerate(submans):
            y = rm.eval_manifold(x)
            y = standardize(y)
            units.append(np.repeat(np.arange(nb_units).reshape(1,-1),nb_samples,axis=0))
            times.append(y.numpy())

        units = np.concatenate(units,axis=1)
        times = np.concatenate(times,axis=1)
        events = np.stack([times,units],axis=2)
        data.append(events)
        labels.append(k*np.ones(len(units)))
        targets.append(x)

    data = np.concatenate(data, axis=0)
    labels = np.array(np.concatenate(labels, axis=0), dtype=int)
    targets = np.concatenate(targets, axis=0)

    if shuffle:
        idx = np.arange(len(data))
        np.random.shuffle(idx)
        data = data[idx]
        labels = labels[idx]
        targets = targets[idx]

    data[:,:,0] *= nb_steps*step_frac
    # data = np.array(data, dtype=int)

    if classification:
        return data, labels
    else:
        return data, targets
    
def events_to_spike_train(data, nb_steps, nb_units):
    """convert the data generated from manifold to spike train form

    Args:
        data (array): shape is [samples, nb_events, 2]

    Returns:
        spike_train: shape is [nb_samples, nb_time_steps, units]
    """
    
    # astyle() will discard the decimal to give integer timestep
    spike_steps = data[:, :, 0].astype(int)
    spike_steps[spike_steps == nb_steps] = nb_steps - 1
    spike_units = data[:, :, 1].astype(int)
    # These will be the indices to entrices in the spike train to be set to 1
    
    # Use the index on spike train matrix [samples, steps, units]
    spike_train = np.zeros((data.shape[0], nb_steps, nb_units))
    sample_indicies = np.expand_dims(np.arange(data.shape[0]), -1)
    spike_train[sample_indicies, spike_steps, spike_units] = 1
    
    return spike_train    

@dataclass
class RandmanConfig:
    nb_classes: int = 10
    nb_units: int = 10
    nb_steps: int = 50
    nb_samples: int = 1000
    dim_manifold: int = 2
    alpha: float = 2.0    
    
    @classmethod
    def lookup_by_id(cls, id: int, table_path="data/randman/meta-data.csv"):
        """
        Lookup a row by id in a CSV file.
        Args:
            table_path (str): Path to the CSV file containing Randman configurations.
            id (int): The ID of the configuration to look up.
        Returns:
            RandmanConfig: An instance of RandmanConfig with parameters from the specified row.
        """
        df = pd.read_csv(table_path, index_col="id")
        if id not in df.index:
            raise ValueError(f"ID {id} not found in {table_path}.")
        kwargs = df.loc[id].to_dict()
        filename = kwargs.pop("filename")

        result = cls(**kwargs)
        result.filename = filename
        return result

    @classmethod
    def delete_by_id(cls, id: int, table_path="data/randman/meta-data.csv"):
        df = pd.read_csv(table_path, index_col="id")
        if id in df.index:
            # remove the corresponding dataset file
            filename = df.loc[id]["filename"]
            filepath = os.path.join(os.path.dirname(table_path), filename)
            if os.path.isfile(filepath):
                os.remove(filepath)
                print(f"Deleted dataset file: {filepath}")
            else:
                print(f"Dataset file not found: {filepath}, but will still remove the entry from CSV.")
            df = df.drop(index=id)
            df.to_csv(table_path, index=True, index_label='id')
            print(f"Deleted entry with ID {id} from {table_path}")
            
        
    def read_dataset(self, save_dir="data/randman"):
        meta_path = os.path.join(save_dir, "meta-data.csv")
        if not os.path.isfile(meta_path):
            raise FileNotFoundError(f"Meta-data file not found at {meta_path}")

        df = pd.read_csv(meta_path)
        match = match_config(df, self)
        if match.empty:
            raise ValueError("No dataset found with the specified parameters.")

        filename = match.iloc[0]["filename"]
        filepath = os.path.join(save_dir, filename)
        if not os.path.isfile(filepath):
            raise FileNotFoundError(f"Dataset file not found at {filepath}")
        print("filepath", filepath)
        
        data = torch.load(filepath, weights_only=False)
        return data

def get_randman_dataset(config: RandmanConfig):
    """
    Generate a spiking neural network dataset using Randman configuration.
    Args:
        config (RandmanConfig): Configuration object containing parameters for
            dataset generation including nb_steps, nb_units, and other settings.
            The 'id' and 'filename' fields are excluded from dataset generation.
    Returns:
        TensorDataset: A PyTorch dataset containing spike train tensors and
            corresponding label tensors, ready for use in neural network training.
    Note:
        - The function generates exactly 1 spike per unit (nb_spikes=1)
    """
    
    data, label = make_spiking_dataset(**asdict(config), nb_spikes=1)
    spike_train = events_to_spike_train(data, config.nb_steps, config.nb_units)
    
    spike_train = torch.Tensor(spike_train)
    label = torch.Tensor(label)
    
    # encapulate using Torch.Dataset
    dataset = TensorDataset(spike_train, label)
    
    return dataset

def generate_and_save_randman(config: RandmanConfig, save_dir="data/randman"):    
    # Ensure the save directory exists
    os.makedirs(save_dir, exist_ok=True)
    meta_path = os.path.join(save_dir, "meta-data.csv")
    
    # convert to dict
    config_dict = asdict(config)
    
    # Check for existing parameter combination
    if os.path.isfile(meta_path):
        df = pd.read_csv(meta_path, index_col='id')
        match = match_config(df, config)
        if not match.empty:
            raise ValueError("Dataset already exists with the same parameters.")

    # Generate dataset
    data = get_randman_dataset(config)
    
    # Generate random filename
    filename = f"{uuid.uuid4().hex}.pt"
    filepath = os.path.join(save_dir, filename)
    config_dict['filename'] = filename
    
    # Save dataset
    torch.save(data, filepath)
    
    # Log parameters to meta-data.csv 
    if os.path.isfile(meta_path):
        df = pd.read_csv(meta_path, index_col='id')
        row_df = pd.DataFrame([config_dict], index=[next_id(df)])
        df = pd.concat([df, row_df], ignore_index=False)
    else:
        # Start with ID 0 for the first entry
        df = pd.DataFrame([config_dict], index=[0])

    df.to_csv(meta_path, index=True, index_label='id')
    return filepath

def split_and_load(data, batch_size):
    tmp_dataset, _ = train_test_split(data, test_size=0.2, shuffle=False)
    train_dataset, val_dataset = train_test_split(tmp_dataset, test_size=0.25, shuffle=False)
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False)
    return train_dataloader, val_dataloader

def split_test_and_load(data):
    '''
        For hyperparameters see read_randman10_dataset().
    '''
    _, test_dataset = train_test_split(data, test_size=0.2, shuffle=False)
    test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False)
    return test_dataloader