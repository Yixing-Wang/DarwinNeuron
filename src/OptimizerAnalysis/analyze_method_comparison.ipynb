{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200fc5d8-8da8-4595-9f95-6174f30955e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1: Imports & Config =====\n",
    "import os, json, sqlite3\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Dict, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可视化（可选）\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML（可选）\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score\n",
    "\n",
    "# ----------------- 配置 -----------------\n",
    "DB = \"data/landscape-analysis.db\"      # 你的 SQLite\n",
    "OUT_ROOT = Path(\"data/MethodComparisonRuns\")\n",
    "PROBLEM_IDS: Iterable[int] = list(range(0, 8))  # 需要分析的 problem 范围\n",
    "TARGET_EPOCHS = 100                    # 你希望的“跑满” epoch 数判定阈值\n",
    "\n",
    "# 可选：如果你已经确定 feature 表名和键列，可以在这里填\n",
    "FEATURE_TABLE_NAME = None              # 例如: \"mla_features\"\n",
    "FEATURE_KEY_COLUMN = None              # 例如: \"problem_id\" 或 \"nn_problem_id\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367c6bbb-fafb-44f7-89aa-a0276a308de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2: DB helpers & gen_log readers =====\n",
    "\n",
    "def _rows_for_problems(db: str, pids: Iterable[int]) -> List[Tuple]:\n",
    "    \"\"\"从 DB 取 run 清单 (run_id, problem_type, problem_id, algo_name, result_filename)\"\"\"\n",
    "    pids = list(pids)\n",
    "    if not pids:\n",
    "        return []\n",
    "    placeholders = \",\".join(\"?\" for _ in pids)\n",
    "    q = f\"\"\"\n",
    "    SELECT r.id, r.problem_type, r.problem_id, a.name, r.result_filename\n",
    "    FROM runs r\n",
    "    JOIN algorithm a ON a.id = r.algorithm_id\n",
    "    WHERE r.problem_type='nn' AND r.problem_id IN ({placeholders})\n",
    "    ORDER BY r.problem_id, a.name, r.id;\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(db) as con:\n",
    "        return con.execute(q, pids).fetchall()\n",
    "\n",
    "def _read_gen_log(log_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    读取 gen_log.csv，兼容你“新列集”：\n",
    "      gen,n_evals,epoch,batch,best_f,best_acc,avg_acc,wall_time,epoch_acc_full,x_file,x_epoch_file\n",
    "    也兼容旧格式（缺的列自动补 NA）。\n",
    "    \"\"\"\n",
    "    # 允许列 superset，但下游分析只用到以下这些：\n",
    "    cols = [\"gen\",\"n_evals\",\"epoch\",\"batch\",\"best_f\",\"best_acc\",\"avg_acc\",\n",
    "            \"wall_time\",\"epoch_acc_full\",\"x_file\",\"x_epoch_file\"]\n",
    "    if not log_path.exists():\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    df = pd.read_csv(log_path)\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    for c in [\"gen\",\"n_evals\",\"epoch\",\"batch\",\"best_f\",\"best_acc\",\"avg_acc\",\"wall_time\",\"epoch_acc_full\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    return df[cols]\n",
    "\n",
    "def _read_result_json(path: Path) -> Dict:\n",
    "    if path.exists():\n",
    "        try:\n",
    "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def _gather_algo_runs(algo_dir: Path) -> Dict[str, Union[List[Tuple[int, Path, Path]], Optional[Path]]]:\n",
    "    \"\"\"\n",
    "    识别一个算法目录下的多 seed 结构：\n",
    "      - seeds: [(seed, gen_csv, result_json), ...]\n",
    "      - avg: algo_dir/gen_log_avg.csv（若存在）\n",
    "      - single: algo_dir/gen_log.csv（单 seed 时）\n",
    "    \"\"\"\n",
    "    out = {\"seeds\": [], \"avg\": None, \"single\": None}\n",
    "\n",
    "    avg_csv = algo_dir / \"gen_log_avg.csv\"\n",
    "    if avg_csv.exists():\n",
    "        out[\"avg\"] = avg_csv\n",
    "\n",
    "    # 多 seed 子目录\n",
    "    for p in sorted(algo_dir.glob(\"seed_*\")):\n",
    "        try:\n",
    "            seed = int(p.name.split(\"_\", 1)[1])\n",
    "        except Exception:\n",
    "            continue\n",
    "        gen_csv = p / \"gen_log.csv\"\n",
    "        res_json = p / \"result.json\"\n",
    "        if gen_csv.exists():\n",
    "            out[\"seeds\"].append((seed, gen_csv, res_json))\n",
    "\n",
    "    # 单 seed\n",
    "    if not out[\"seeds\"]:\n",
    "        gen_csv = algo_dir / \"gen_log.csv\"\n",
    "        if gen_csv.exists():\n",
    "            out[\"single\"] = gen_csv\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c028e01-118e-4575-8d74-6aada41bdd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/wx2178/SNN/DarwinNeuron\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/wx2178/.conda/envs/cns/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915ba4a-875f-4357-aa48-bdc9e1677296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3: Load all gen logs (with seeds) into one long table =====\n",
    "\n",
    "def load_all_runs_long(db: str, pids: Iterable[int], out_root: Path) -> pd.DataFrame:\n",
    "    rows = _rows_for_problems(db, pids)\n",
    "    records = []\n",
    "\n",
    "    for run_id, ptype, pid, algoname, result_file in rows:\n",
    "        algo = algoname.upper()\n",
    "        run_dir = Path(result_file).parent           # 可能是 .../ALGO 或 .../ALGO/seed_S\n",
    "        algo_dir = run_dir if run_dir.name.upper() == algo else run_dir.parent\n",
    "        layout = _gather_algo_runs(algo_dir)\n",
    "\n",
    "        if layout[\"seeds\"]:\n",
    "            # 多 seed：逐 seed 读取\n",
    "            for seed, gen_csv, _res_json in layout[\"seeds\"]:\n",
    "                df = _read_gen_log(gen_csv)\n",
    "                if df.empty: \n",
    "                    continue\n",
    "                df.insert(0, \"seed\", seed)\n",
    "                df.insert(0, \"algorithm\", algo)\n",
    "                df.insert(0, \"problem_id\", pid)\n",
    "                df.insert(0, \"run_id\", run_id)\n",
    "                records.append(df)\n",
    "\n",
    "            # 平均曲线（若需要画图或参考）\n",
    "            if layout[\"avg\"] is not None and layout[\"avg\"].exists():\n",
    "                dfa = _read_gen_log(layout[\"avg\"])\n",
    "                if not dfa.empty:\n",
    "                    dfa.insert(0, \"seed\", \"avg\")\n",
    "                    dfa.insert(0, \"algorithm\", algo)\n",
    "                    dfa.insert(0, \"problem_id\", pid)\n",
    "                    dfa.insert(0, \"run_id\", run_id)\n",
    "                    records.append(dfa)\n",
    "        else:\n",
    "            # 单 seed\n",
    "            gen_csv = layout[\"single\"]\n",
    "            if gen_csv is not None and gen_csv.exists():\n",
    "                df = _read_gen_log(gen_csv)\n",
    "                if not df.empty:\n",
    "                    df.insert(0, \"seed\", 0)\n",
    "                    df.insert(0, \"algorithm\", algo)\n",
    "                    df.insert(0, \"problem_id\", pid)\n",
    "                    df.insert(0, \"run_id\", run_id)\n",
    "                    records.append(df)\n",
    "\n",
    "    if not records:\n",
    "        cols = [\"run_id\",\"problem_id\",\"algorithm\",\"seed\",\"gen\",\"n_evals\",\"epoch\",\"batch\",\n",
    "                \"best_f\",\"best_acc\",\"avg_acc\",\"wall_time\",\"epoch_acc_full\",\"x_file\",\"x_epoch_file\"]\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    out = pd.concat(records, ignore_index=True)\n",
    "    # 存一下长表（分 problem 存也行）\n",
    "    (out_root / \"analysis\").mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(out_root / \"analysis\" / \"all_runs_long.csv\", index=False)\n",
    "    out\n",
    "    return out\n",
    "\n",
    "df_long = load_all_runs_long(DB, PROBLEM_IDS, OUT_ROOT)\n",
    "df_long.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee9fdc93-0a52-4913-a123-39291e18eb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary to data/MethodComparisonRuns/analysis/runs_summary.csv, shape=(28208, 16)\n",
      "      run_id  problem_id algorithm seed  gen  n_evals  epoch  batch    best_f  \\\n",
      "7636     252           0     CMAES    0    2       70      0      2  0.891866   \n",
      "7639     252           0     CMAES    0    5      220      1      2  0.686298   \n",
      "7642     252           0     CMAES    0    8      370      2      2  0.553679   \n",
      "7645     252           0     CMAES    0   11      520      3      2  0.484145   \n",
      "7648     252           0     CMAES    0   14      670      4      2  0.438862   \n",
      "\n",
      "      best_acc   avg_acc   wall_time  epoch_acc_full  \\\n",
      "7636  0.529070  0.532248   33.970935        0.505000   \n",
      "7639  0.534884  0.518992  107.090044        0.535000   \n",
      "7642  0.635659  0.583992  186.231977        0.597500   \n",
      "7645  0.693798  0.605620  264.746003        0.692500   \n",
      "7648  0.734496  0.639961  341.486718        0.716667   \n",
      "\n",
      "                                                 x_file  \\\n",
      "7636  data/MethodComparisonRuns/NN_0/CMAES/seed_0/be...   \n",
      "7639  data/MethodComparisonRuns/NN_0/CMAES/seed_0/be...   \n",
      "7642  data/MethodComparisonRuns/NN_0/CMAES/seed_0/be...   \n",
      "7645  data/MethodComparisonRuns/NN_0/CMAES/seed_0/be...   \n",
      "7648  data/MethodComparisonRuns/NN_0/CMAES/seed_0/be...   \n",
      "\n",
      "                                           x_epoch_file  epoch_acc_full_filled  \n",
      "7636  data/MethodComparisonRuns/NN_0/CMAES/seed_0/be...               0.505000  \n",
      "7639  data/MethodComparisonRuns/NN_0/CMAES/seed_0/be...               0.535000  \n",
      "7642  data/MethodComparisonRuns/NN_0/CMAES/seed_0/be...               0.597500  \n",
      "7645  data/MethodComparisonRuns/NN_0/CMAES/seed_0/be...               0.692500  \n",
      "7648  data/MethodComparisonRuns/NN_0/CMAES/seed_0/be...               0.716667  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_rows = \"data/MethodComparisonRuns/analysis/all_runs_long.csv\"\n",
    "output_file = \"data/MethodComparisonRuns/analysis/runs_summary.csv\"\n",
    "\n",
    "df = pd.read_csv(all_rows)\n",
    "\n",
    "# 1) 每个 (problem_id, algorithm, seed, epoch) 取 gen 最大的一行\n",
    "last_per_epoch = (\n",
    "    df.sort_values([\"problem_id\", \"algorithm\", \"seed\", \"epoch\", \"gen\"])\n",
    "      .groupby([\"problem_id\", \"algorithm\", \"seed\", \"epoch\"], as_index=False)\n",
    "      .tail(1)\n",
    ")\n",
    "\n",
    "# 2) 给 epoch_acc_full 补最近的非空值\n",
    "# 方法：对每个组合 (problem_id, algorithm, seed)，按 epoch 升序，\n",
    "#      用 ffill(bfill) 填充 epoch_acc_full\n",
    "last_per_epoch[\"epoch_acc_full_filled\"] = (\n",
    "    last_per_epoch.sort_values([\"problem_id\", \"algorithm\", \"seed\", \"epoch\"])\n",
    "    .groupby([\"problem_id\", \"algorithm\", \"seed\"])[\"epoch_acc_full\"]\n",
    "    .ffill()\n",
    "    .bfill()\n",
    ")\n",
    "\n",
    "# 3) 保存结果\n",
    "last_per_epoch.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Saved summary to {output_file}, shape={last_per_epoch.shape}\")\n",
    "print(last_per_epoch.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31409ebd-0f22-437c-8867-81420e2cf8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows written to data/MethodComparisonRuns/analysis/runs_summary_short.csv, shape=(215, 16)\n",
      "     run_id  problem_id algorithm seed  gen  n_evals  epoch  batch    best_f  \\\n",
      "100     252           0     CMAES    0  300    14970    100      0  0.123908   \n",
      "201     252           0     CMAES    1  300    14970    100      0  0.107587   \n",
      "378     252           0     CMAES    5  300    14970    100      0  0.106792   \n",
      "479     252           0     CMAES    6  300    14970    100      0  0.151216   \n",
      "615     249           0        DE    0  300    15000    100      0  0.402712   \n",
      "\n",
      "     best_acc   avg_acc    wall_time  epoch_acc_full  \\\n",
      "100  0.928571  0.795000  7674.187000             NaN   \n",
      "201  0.916667  0.845238  8079.311826             NaN   \n",
      "378  0.922619  0.887976  1981.259895             NaN   \n",
      "479  0.916667  0.764881  1967.786267             NaN   \n",
      "615  0.660714  0.655952  5189.215234             NaN   \n",
      "\n",
      "                                                x_file x_epoch_file  \\\n",
      "100  data/MethodComparisonRuns/NN_0/CMAES/seed_0/be...          NaN   \n",
      "201  data/MethodComparisonRuns/NN_0/CMAES/seed_1/be...          NaN   \n",
      "378  data/MethodComparisonRuns/NN_0/CMAES/seed_5/be...          NaN   \n",
      "479  data/MethodComparisonRuns/NN_0/CMAES/seed_6/be...          NaN   \n",
      "615  data/MethodComparisonRuns/NN_0/DE/seed_0/best_...          NaN   \n",
      "\n",
      "     epoch_acc_full_filled  \n",
      "100               0.875000  \n",
      "201               0.871667  \n",
      "378               0.901667  \n",
      "479               0.871667  \n",
      "615               0.635000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_rows = \"data/MethodComparisonRuns/analysis/runs_summary.csv\"\n",
    "output_file = \"data/MethodComparisonRuns/analysis/runs_summary_short.csv\"\n",
    "\n",
    "# 读入 CSV\n",
    "df = pd.read_csv(all_rows)\n",
    "\n",
    "# 只保留 epoch 等于 99 或 100 的行\n",
    "df_filtered = pd.concat([\n",
    "    df[df[\"epoch\"] == 100],\n",
    "    df[(df[\"algorithm\"] == \"SGD\") & (df[\"epoch\"] == 99)]\n",
    "])\n",
    "df_filtered = df_filtered[df_filtered[\"seed\"] != \"avg\"].copy()\n",
    "\n",
    "# 存成新的 CSV\n",
    "df_filtered.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered rows written to {output_file}, shape={df_filtered.shape}\")\n",
    "print(df_filtered.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f0e8589-51d7-4728-8e20-3e61d2566500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (216, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_id</th>\n",
       "      <th>function_idx</th>\n",
       "      <th>instance_idx</th>\n",
       "      <th>dim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   problem_id  function_idx  instance_idx  dim\n",
       "0           1             1             1    2\n",
       "1           2             1             2    2\n",
       "2           3             1             3    2\n",
       "3           4             1             1  160\n",
       "4           5             1             2  160"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5: Load landscape features from SQLite =====\n",
    "\n",
    "def list_tables(db: str) -> List[str]:\n",
    "    with sqlite3.connect(db) as con:\n",
    "        q = \"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\"\n",
    "        return [r[0] for r in con.execute(q).fetchall()]\n",
    "\n",
    "def infer_feature_table_and_key(db: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    尝试寻找一个包含 problem id 的“特征表”：\n",
    "      - 优先包含列：problem_id 或 nn_problem_id\n",
    "      - 排除运行表：runs/algorithm/termination 等\n",
    "    \"\"\"\n",
    "    cand_keys = [\"problem_id\",\"nn_problem_id\",\"pid\",\"id\"]\n",
    "    bad_names = {\"runs\",\"run\",\"algorithm\",\"termination\",\"gen_log\",\"result\",\"pymoo_history\"}\n",
    "    tbs = list_tables(db)\n",
    "    for tb in tbs:\n",
    "        if tb.lower() in bad_names:\n",
    "            continue\n",
    "        try:\n",
    "            with sqlite3.connect(db) as con:\n",
    "                info = con.execute(f\"PRAGMA table_info({tb});\").fetchall()\n",
    "                cols = [r[1] for r in info]\n",
    "        except Exception:\n",
    "            continue\n",
    "        key = None\n",
    "        for k in cand_keys:\n",
    "            if k in cols:\n",
    "                key = k\n",
    "                break\n",
    "        if key is not None:\n",
    "            # 进一步简单启发式：包含一些“明显是特征”的列名\n",
    "            feature_like = [c for c in cols if c not in {key} and not c.endswith(\"_id\")]\n",
    "            if len(feature_like) >= 2:\n",
    "                return tb, key\n",
    "    return None, None\n",
    "\n",
    "def load_features(db: str,\n",
    "                  table_name: Optional[str] = None,\n",
    "                  key_col: Optional[str] = None) -> pd.DataFrame:\n",
    "    if table_name is None or key_col is None:\n",
    "        tb, key = infer_feature_table_and_key(db)\n",
    "        table_name = table_name or tb\n",
    "        key_col = key_col or key\n",
    "\n",
    "    if not table_name or not key_col:\n",
    "        raise RuntimeError(\"无法自动推断 feature 表名/键列。请手工设置 FEATURE_TABLE_NAME 和 FEATURE_KEY_COLUMN。\")\n",
    "\n",
    "    with sqlite3.connect(db) as con:\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", con)\n",
    "\n",
    "    # 规范 key 名为 problem_id\n",
    "    if key_col != \"problem_id\":\n",
    "        df = df.rename(columns={key_col: \"problem_id\"})\n",
    "    return df\n",
    "\n",
    "# 自动/显式加载\n",
    "feat_df = load_features(DB, FEATURE_TABLE_NAME, FEATURE_KEY_COLUMN)\n",
    "print(\"features shape:\", feat_df.shape)\n",
    "feat_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39261095-5ae9-4d11-aeb7-0d44a4623bd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 1) 取问题级标签（最佳算法） & 合并 features\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m label_df \u001b[38;5;241m=\u001b[39m best_algo_per_problem(\u001b[43mavg_df\u001b[49m)\n\u001b[1;32m     29\u001b[0m meta_cls_df \u001b[38;5;241m=\u001b[39m feat_df\u001b[38;5;241m.\u001b[39mmerge(label_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproblem_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta classification table:\u001b[39m\u001b[38;5;124m\"\u001b[39m, meta_cls_df\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_df' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6: Merge features & performance, do correlations and baselines =====\n",
    "\n",
    "def best_algo_per_problem(avg_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"为每个 problem 选出 mean_best_f 最小的算法，得到 (problem, best_algo, best_f, best_acc, ...)\"\"\"\n",
    "    rows = []\n",
    "    for pid, g in avg_df.groupby(\"problem\"):\n",
    "        g2 = g.sort_values(\"mean_best_f\", ascending=True).iloc[0]\n",
    "        rows.append({\n",
    "            \"problem_id\": pid,\n",
    "            \"best_algo\":  g2[\"algorithm\"],\n",
    "            \"best_f\":     g2[\"mean_best_f\"],\n",
    "            \"best_acc\":   g2[\"mean_best_acc\"],\n",
    "            \"best_wall\":  g2[\"mean_wall_time\"],\n",
    "            \"finished_ratio\": g2[\"finished_ratio\"],\n",
    "            \"epochs_done_mean\": g2[\"epochs_done_mean\"],\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def melt_algo_performance(avg_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    另一种：把 (problem, algorithm) 的均值性能保留，用来做 per-algorithm 回归。\n",
    "    返回列：problem_id, algorithm, mean_best_f, mean_best_acc, ...\n",
    "    \"\"\"\n",
    "    m = avg_df.rename(columns={\"problem\":\"problem_id\"}).copy()\n",
    "    return m\n",
    "\n",
    "# 1) 取问题级标签（最佳算法） & 合并 features\n",
    "label_df = best_algo_per_problem(avg_df)\n",
    "meta_cls_df = feat_df.merge(label_df, on=\"problem_id\", how=\"inner\")\n",
    "print(\"meta classification table:\", meta_cls_df.shape)\n",
    "meta_cls_df.head()\n",
    "\n",
    "# 2) 简单相关性（features ↔ best_f / best_acc）\n",
    "num_cols = meta_cls_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr_target_cols = [\"best_f\",\"best_acc\"]\n",
    "corrs = {}\n",
    "for tgt in corr_target_cols:\n",
    "    if tgt in meta_cls_df.columns:\n",
    "        corrs[tgt] = meta_cls_df[num_cols].corr()[tgt].sort_values(ascending=False)\n",
    "        print(f\"\\n=== Correlations with {tgt} ===\")\n",
    "        display(corrs[tgt].to_frame(f\"corr_with_{tgt}\"))\n",
    "\n",
    "# 3) Leave-One-Problem-Out 分类 baseline：预测 best_algo\n",
    "if len(meta_cls_df) >= 3:\n",
    "    # 准备特征/标签\n",
    "    feature_cols = [c for c in meta_cls_df.columns \n",
    "                    if c not in {\"problem_id\",\"best_algo\",\"best_f\",\"best_acc\",\"best_wall\",\"finished_ratio\",\"epochs_done_mean\"} \n",
    "                    and pd.api.types.is_numeric_dtype(meta_cls_df[c])]\n",
    "    X = meta_cls_df[feature_cols].values\n",
    "    y = meta_cls_df[\"best_algo\"].values\n",
    "\n",
    "    # LOO-CV\n",
    "    uniq_probs = meta_cls_df[\"problem_id\"].unique()\n",
    "    preds, trues = [], []\n",
    "    for pid in uniq_probs:\n",
    "        train_idx = meta_cls_df[\"problem_id\"] != pid\n",
    "        test_idx  = meta_cls_df[\"problem_id\"] == pid\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "            (\"clf\", RandomForestClassifier(n_estimators=300, random_state=42))\n",
    "        ])\n",
    "        pipe.fit(X[train_idx], y[train_idx])\n",
    "        yhat = pipe.predict(X[test_idx])\n",
    "        preds.extend(yhat.tolist())\n",
    "        trues.extend(y[test_idx].tolist())\n",
    "\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    print(f\"\\n[LOO] Best-optimizer classification accuracy = {acc:.3f}  \"\n",
    "          f\"(n_problems={len(uniq_probs)}, classes={len(np.unique(y))})\")\n",
    "\n",
    "# 4) 每算法回归 baseline（预测 mean_best_f）\n",
    "perf_long = melt_algo_performance(avg_df)     # problem_id, algorithm, mean_best_f, ...\n",
    "meta_reg_df = perf_long.merge(feat_df, on=\"problem_id\", how=\"inner\")\n",
    "\n",
    "if len(meta_reg_df[\"problem_id\"].unique()) >= 3:\n",
    "    feature_cols = [c for c in meta_reg_df.columns \n",
    "                    if c not in {\"problem_id\",\"algorithm\",\"mean_best_f\",\"mean_best_acc\",\"mean_epoch_acc_full\",\n",
    "                                 \"mean_wall_time\",\"seeds\",\"finished_ratio\",\"epochs_done_mean\"}\n",
    "                    and pd.api.types.is_numeric_dtype(meta_reg_df[c])]\n",
    "\n",
    "    uniq_probs = meta_reg_df[\"problem_id\"].unique()\n",
    "    maes, r2s = [], []\n",
    "    for pid in uniq_probs:\n",
    "        trn = meta_reg_df[\"problem_id\"] != pid\n",
    "        tst = meta_reg_df[\"problem_id\"] == pid\n",
    "\n",
    "        Xtr = meta_reg_df.loc[trn, feature_cols].values\n",
    "        ytr = meta_reg_df.loc[trn, \"mean_best_f\"].values\n",
    "        Xte = meta_reg_df.loc[tst, feature_cols].values\n",
    "        yte = meta_reg_df.loc[tst, \"mean_best_f\"].values\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "            (\"reg\", RandomForestRegressor(n_estimators=400, random_state=42))\n",
    "        ])\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        yhat = pipe.predict(Xte)\n",
    "        maes.append(mean_absolute_error(yte, yhat))\n",
    "        r2s.append(r2_score(yte, yhat))\n",
    "\n",
    "    print(f\"[LOO] mean_best_f regression  |  MAE={np.mean(maes):.4f}  R^2={np.mean(r2s):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c1ec4-43f2-4e31-b629-1dfc0501250f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cns)",
   "language": "python",
   "name": "cns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
